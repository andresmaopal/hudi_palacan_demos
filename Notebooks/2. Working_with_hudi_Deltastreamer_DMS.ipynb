{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with AWS DMS, Athena and Apache Hudi Deltastreamer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn to use DeltaStreamer Utility to bulk insert data into a Hudi Dataset as a Copy on Write(CoW) and Merge on Write (MOR) using a DMS Full load and CDC task as a source.  \n",
    "\n",
    "We will run queries in hudi-cli and SparkSQL to verify the tables and subsequent updates are incorporated into our datalake on Amazon S3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the setup of the DMS environment (first notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.ibb.co/7nPtdfG/hudi-s1.png\" alt=\"hudi-s1\" border=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python AWS Wrangler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to write a Copy on Write (COW) table\n",
    "\n",
    "We will now run the DeltaStreamer utility as an EMR Step to write the above JSON formatted data into a Hudi dataset. To do that, we will need the following:\n",
    "\n",
    "* Properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2f9d7006b9414eb5c7684df6eddb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "boto3.setup_default_session(region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the bulk insert job to create Copy on Write (COW) table\n",
    "\n",
    "### Bulk insert from parquet source table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221ad64ab2c144488b48f6c24617d47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  --jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  --master yarn  file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  --table-type COPY_ON_WRITE  --op BULK_INSERT --source-class org.apache.hudi.utilities.sources.ParquetDFSSource  --source-ordering-field op_cdc_timestamp  --target-base-path s3://hudi-dms-repl-dmslabs3bucket-1q0p4wdv4oi0j/dms_sample/hudi_sporting_event_ticket/  --target-table hudi_sporting_event_ticket  --transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  --payload-class org.apache.hudi.payload.AWSDmsAvroPayload  --enable-hive-sync  --hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://hudi-dms-repl-dmslabs3bucket-1q0p4wdv4oi0j/dms_sample/sporting_event_ticket/   --hoodie-conf hoodie.datasource.write.recordkey.field=id  --hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row --hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row --hoodie-conf hoodie.datasource.hive_sync.database=default  --hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket   --hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \n",
      "-------------\n",
      "STEP s-1F8WEPABM4GFA initiated on Cluster j-3PNYPQ5J3JROS"
     ]
    }
   ],
   "source": [
    "#COW\n",
    "# SOURCE TABLE sporting_event_ticket\n",
    "\n",
    "cluster_id = \"j-XXXXX\"\n",
    "\n",
    "source_bucket=\"hudi-dms-repl-XXXXXX\"\n",
    "source_key=\"cdc/dms_sample/sporting_event_ticket/\"\n",
    "target_key=\"cdc/dms_sample/hudi_sporting_event_ticket/\"\n",
    "target_table=\"hudi_sporting_event_ticket\"\n",
    "ordering_field=\"op_cdc_timestamp\"\n",
    "id_field=\"id\"\n",
    "partition_field=\"seat_row\"\n",
    "\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type COPY_ON_WRITE  \\\n",
    "--continuous\n",
    "-- mac 60\n",
    "--op UPSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field \"+ordering_field+\"  \\\n",
    "--target-base-path s3://\"+source_bucket+\"/\"+target_key+\"  \\\n",
    "--target-table \"+target_table+\"  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://\"+source_bucket+\"/\"+source_key+\"   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=\"+id_field+\"  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=\"+target_table+\"   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \\\n",
    "\"\n",
    "print(command)\n",
    "print(\"-------------\")\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(\"STEP {} initiated on Cluster {}\".format(step_id,cluster_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2bf7d551bc4359953d250f4c406116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PENDING"
     ]
    }
   ],
   "source": [
    "#Check the status of the step\n",
    "\n",
    "print(wr.emr.get_step_state(cluster_id, step_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets verify the records on Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to <strong>Athena</strong>, and in the tables list (left panel) select the database <strong>\"default\"</strong>, you can now see the table \"hudi_sporting_event_ticket\" as part of the Glue Data Catalog\n",
    "\n",
    "2. Execute the following query to filter some example records and see the outputs.\n",
    "\n",
    "<ul>\n",
    "<p style=\"color:BLUE\">\n",
    "SELECT id,ticket_price FROM \"default\".\"hudi_sporting_event_ticket\" where id > 1 and id < 51;\n",
    "</p>\n",
    "    \n",
    "<a href=\"https://ibb.co/yRsSJ95\"><img src=\"https://i.ibb.co/g7gVC2w/athena-screenshot-2.png\" alt=\"athena-screenshot-2\" border=\"0\"></a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do some updates on the source Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN;   \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '1';  \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '11'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '21'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '31'; \\\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1cdd084b0e4b3c874fce89cfb41dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  --jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  --master yarn  file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  --table-type COPY_ON_WRITE  --op UPSERT --source-class org.apache.hudi.utilities.sources.ParquetDFSSource  --source-ordering-field op_cdc_timestamp  --target-base-path s3://hudi-dms-repl-dmslabs3bucket-1q0p4wdv4oi0j/dms_sample/hudi_sporting_event_ticket/  --target-table hudi_sporting_event_ticket  --transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  --payload-class org.apache.hudi.payload.AWSDmsAvroPayload  --enable-hive-sync  --hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://hudi-dms-repl-dmslabs3bucket-1q0p4wdv4oi0j/dms_sample/sporting_event_ticket/   --hoodie-conf hoodie.datasource.write.recordkey.field=id  --hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row --hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row --hoodie-conf hoodie.datasource.hive_sync.database=default  --hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket   --hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \n",
      "-------------\n",
      "STEP s-3D2AQW0VAICZA initiated on Cluster j-3PNYPQ5J3JROS"
     ]
    }
   ],
   "source": [
    "#UPSERT COW\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type COPY_ON_WRITE  \\\n",
    "--op UPSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field \"+ordering_field+\"  \\\n",
    "--target-base-path s3://\"+source_bucket+\"/\"+target_key+\"  \\\n",
    "--target-table \"+target_table+\"  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://\"+source_bucket+\"/\"+source_key+\"   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=\"+id_field+\"  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=\"+target_table+\"   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \\\n",
    "\"\n",
    "print(command)\n",
    "print(\"-------------\")\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id7 = wr.emr.submit_step(cluster_id, command)\n",
    "print(\"STEP {} initiated on Cluster {}\".format(step_id7,cluster_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c52ec191e54c9a8fabf509eeaed775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED"
     ]
    }
   ],
   "source": [
    "print(wr.emr.get_step_state(cluster_id, step_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to create Merge on Read (MOR) table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8def8d7af84919bafaae8bb7551fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP s-J0DVYZOCOO6S initiated on Cluster j-3PNYPQ5J3JROS"
     ]
    }
   ],
   "source": [
    "#MOR\n",
    "# TABLE sporting_event\n",
    "\n",
    "source_key=\"cdc/dms_sample/sporting_event_ticket/\"\n",
    "target_key=\"cdc/dms_sample/hudi_sporting_event_ticket_mor/\"\n",
    "target_table=\"hudi_sporting_event_ticket_mor\"\n",
    "ordering_field=\"op_cdc_timestamp\"\n",
    "id_field=\"id\"\n",
    "partition_field=\"seat_row\"\n",
    "\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle.jar  \\\n",
    "--table-type MERGE_ON_READ  \\\n",
    "--op INSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field \"+ordering_field+\"  \\\n",
    "--target-base-path s3://\"+source_bucket+\"/\"+target_key+\"  \\\n",
    "--target-table \"+target_table+\"  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://\"+source_bucket+\"/\"+source_key+\"   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=\"+id_field+\"  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=\"+target_table+\"   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \\\n",
    "\"\n",
    "#print(command)\n",
    "\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id3 = wr.emr.submit_step(cluster_id, command)\n",
    "print(\"STEP {} initiated on Cluster {}\".format(step_id3,cluster_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81654a97db7e471aac5b6f8b9e1df859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED"
     ]
    }
   ],
   "source": [
    "#Check the status of the step\n",
    "\n",
    "print(wr.emr.get_step_state(cluster_id, step_id3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait the step to finish and query the generated table on Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some CDC updates on the source Postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s generate some CDC data, by running updates now on the fake profile data generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN;   \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '1';  \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '11'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '21'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '31'; \\\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a DeltaStreamer upsert job on MOR Table to update the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a51e6d0e9e4e5b8db4ff1d893892e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP s-3UZ8XM2QJD92M initiated on Cluster j-3PNYPQ5J3JROS"
     ]
    }
   ],
   "source": [
    "# SOURCE TABLE sporting_event_ticket \n",
    "\n",
    "target_key=\"cdc/dms_sample/hudi_sporting_event_ticket_mor/\"\n",
    "target_table=\"cdc/hudi_sporting_event_ticket_mor\"\n",
    "ordering_field=\"op_cdc_timestamp\"\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle.jar  \\\n",
    "--table-type MERGE_ON_READ  \\\n",
    "--op UPSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field \"+ordering_field+\"  \\\n",
    "--target-base-path s3://\"+source_bucket+\"/\"+target_key+\"  \\\n",
    "--target-table \"+target_table+\"  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://\"+source_bucket+\"/\"+source_key+\"   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=\"+id_field+\"  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=\"+partition_field+\" \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=\"+target_table+\"   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \\\n",
    "\"\n",
    "#print(command)\n",
    "\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id5 = wr.emr.submit_step(cluster_id, command)\n",
    "print(\"STEP {} initiated on Cluster {}\".format(step_id5,cluster_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run again the SQL statement on Athena to verify the updated records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Execute the same above query to filter some example records and verify the UPDATES.\n",
    "\n",
    "<ul>\n",
    "<p style=\"color:BLUE\">\n",
    "SELECT * FROM \"default\".\"hudi_sporting_event_ticket\" where id > 1 and id < 51;\n",
    "</p>\n",
    "\n",
    "<a href=\"https://ibb.co/yRsSJ95\"><img src=\"https://i.ibb.co/g7gVC2w/athena-screenshot-2.png\" alt=\"athena-screenshot-2\" border=\"0\"></a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, check the Hudi CLI to list the different COMMITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check out Hudi CLI\n",
    "\n",
    "Execute on the Terminal: <strong> hudi-cli </strong> to access the HUDI CLI, and execute the following commands:\n",
    "\n",
    "<strong>hudi:</strong> connect --path s3://XXXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/\n",
    "\n",
    "<strong>hudi: hudi_sporting_event_ticket -> </strong> commits show\n",
    "\n",
    "See the different commits (you can rollback to a specific one)\n",
    "\n",
    "```\n",
    "20/05/06 06:47:14 INFO timeline.HoodieActiveTimeline: Loadedinstants java.util.stream.ReferencePipeline$Head@d689f6\n",
    "20/05/06 06:47:15 INFO s3n.S3NativeFileSystem: Opening 's3://XXXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/.hoodie/20200506063222.commit' for reading\n",
    "20/05/06 06:47:15 INFO s3n.S3NativeFileSystem: Opening 's3://XXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/.hoodie/20200506052917.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200506063222 │ 21.6 MB             │ 0                 │  4                  │ 1                        │ 150000                │ 140000                       │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200506052917 │ 21.6 MB             │ 10                │ 0                   │ 1                        │ 150000                │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
