{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hudi lab : batch and incremental updates to a S3 Data lake using Apache Hudi and AWS Database Migration Service\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write and Merge-On-Read tables to an S3 Datalake from data replicated by AWS DMS (Database Migration Service):\n",
    "\n",
    "## Solution overview\n",
    "\n",
    "<img src=\"https://i.ibb.co/34LXyRT/hudi-arq1.png\" alt=\"hudi-arq1\" style=\"width: 700px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop 1. Create the ingestion stack with DMS and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Deploy the following CloudFormation on Oregon (us-west-2) to create a DMS instance and task with the database endpoint provided by the instructor:\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=hudi-dms-repl&templateURL=https://public-slides-bucket.s3.amazonaws.com/cf_templates/dms_instance_tasks_cdc.json\">\n",
    "<img src=\"https://i.ibb.co/X3MTqpC/00-deploy-to-aws.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "* Wait 5-6 minutes for the CloudFormation to be in Complete status, and go to [DMS Tasks](https://us-west-2.console.aws.amazon.com/dms/v2/home?region=us-west-2#tasks)\n",
    "\n",
    "<img src=\"https://i.ibb.co/ZLYQ24r/hudi-s1.png\" alt=\"DMS tasks\" style=\"width: 700px;\"/>\n",
    "\n",
    "* Wait the Full Load task to fully complete \n",
    "\n",
    "* Proceed to the Workshop 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop 2. Create an EMR Cluster as a work environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a key pair in **Oregon (us-west-2)** ###\n",
    "\n",
    "* Go to [Key pairs](https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#KeyPairs:) console and click on **\"Create key pair\"**:\n",
    "\n",
    "  If you are in a **Windows** machine please **download and save** the key as **ppk file**. If you are in a **Mac** or **Linux** please download the key as **pem** file.\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/pbsyNtN/hudi-s9.png\" alt=\"hudi-s9\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "### Create an EMR Cluster ###\n",
    "* Go to [EMR Console](ttps://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#cluster-list:) and choose \"Go to advanced options\" and create a Cluster with the following specifications (EMR 6.1.0 with **Spark, Hive, Presto and Livy**) and enable (check the options) for **Glue catalog** on Spark, Hive and Presto:\n",
    "\n",
    "<img src=\"https://i.ibb.co/54jnNvb/hudi-x1.png\" alt=\"hudi-s2\" style=\"width: 700px;\">\n",
    "\n",
    "\n",
    "* We need to copy Hudi packages from local to HDFS, for that we are going to execute a Step. On the section **Steps (optional)** click on the dropdown list **Step Type** and select **Custom JAR**\n",
    "\n",
    "<img src=\"https://i.ibb.co/t4GNw3Z/hudi-x2.png\" alt=\"hudi-x2\" border=\"0\">\n",
    "\n",
    "* On **JAR location** copy and paste this path:\n",
    "\n",
    "```\n",
    "s3://us-west-2.elasticmapreduce/libs/script-runner/script-runner.jar\n",
    "```\n",
    "* And in **Arguments** copy and paste this path, as the below image shows:\n",
    "\n",
    "```\n",
    "s3://public-slides-bucket/scripts/hudi_copy_step.sh\n",
    "```\n",
    "\n",
    "<img src=\"https://i.ibb.co/fYPG4Gv/hudi-x3.png\" alt=\"hudi-x3\" border=\"0\">\n",
    "\n",
    "\n",
    "* Click on ***Next** and leave everything with the default **1 master** node **m5.xlarge** and **2 core** nodes **m5.xlarge**:\n",
    "\n",
    "<img src=\"https://i.ibb.co/B3D1wTg/hudi-s3.png\" alt=\"hudi-s3\" style=\"width: 700px;\">\n",
    "\n",
    "* Click on **\"Next\"**, leave everything as default and click next, fill the cluster name: \"Hudi cluster\"\n",
    "\n",
    "<img src=\"https://i.ibb.co/7KP9Gmk/hudi-s4.png\" alt=\"hudi-s4\" style=\"width: 500px;\">\n",
    "\n",
    "* Click on **\"Next\"**, specify the previously created EC2 key **\"emr-oregon\"**\n",
    "\n",
    "<img src=\"https://i.ibb.co/9hn367t/hudi-s10.png\" alt=\"hudi-s10\" border=\"0\">\n",
    "\n",
    "\n",
    "* While the cluster is being created, **download the zip** fill with the Jupyter Notebooks for the Lab: [Right click and save link as...](http://xxx.com/zip)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an EMR Notebook to execute the lab\n",
    "\n",
    "* When the cluster is created (Waiting state), got to [EMR Notebooks](https://us-west-2.console.aws.amazon.com/elasticmapreduce/home?region=us-west-2#notebooks-list:)\n",
    "\n",
    "<img src=\"https://i.ibb.co/vBwmDCx/hudi-s5.png\" alt=\"hudi-s5\" style=\"width: 400px;\">\n",
    "\n",
    "* Create a notebook and providing the following options selecting the already created notebook>:\n",
    "\n",
    "<img src=\"https://i.ibb.co/QYT9Lhd/hudi-s6.png\" alt=\"hudi-s6\" style=\"width: 700px;\">\n",
    "\n",
    "* Once the notebook move from \"Starting\" to \"Ready\" state, select it and click on \"Open in Jupyter\":\n",
    "\n",
    "<img src=\"https://i.ibb.co/4V7501s/hudi-s7.png\" alt=\"hudi-s7\" border=\"0\" style=\"width: 600px;\">\n",
    "\n",
    "* It will open a new Tab with a secure pre-signed url for the notebook environment, once there, upload the 2 .ipynb files that were previously downloaded.\n",
    "\n",
    "<img src=\"https://i.ibb.co/GxKBT39/hudi-s8.png\" alt=\"hudi-s8\" style=\"width: 600px;\">\n",
    "\n",
    "* Open the first Notebook: <b>\"1. Working_with_Hudi_Pyspark_Data_Source_API.ipynb\"</b> and complete all the steps\n",
    "\n",
    "<p>.</p>\n",
    "\n",
    "* #### (If deployed the DMS Workshop 1 -- Optional)\n",
    "\n",
    "* When finish, open the second Notebook: <b>\"2. Working_With_Hudi_Deltastreamer.ipynb\"</b> and complete all the steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
