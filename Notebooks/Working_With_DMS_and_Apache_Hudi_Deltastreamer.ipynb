{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with AWS DMS, Athena and Apache Hudi Deltastreamer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will learn to use DeltaStreamer Utility to bulk insert data into a Hudi Dataset as a Copy on Write(CoW) and Merge on Write (MOR) using a DMS Full load and CDC task as a source.  \n",
    "\n",
    "We will run queries in hudi-cli and SparkSQL to verify the tables and subsequent updates are incorporated into our datalake on Amazon S3\n",
    "\n",
    "Let's get started !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment requirements\n",
    "\n",
    "This is designed for EMR notebook on a Test EMR cluster:  \n",
    "\n",
    "- EMR release 5.30.0\n",
    "- Size: 1 master node with 1 core node (r5.4xlarge)\n",
    "- Software installed: Hadoop,Spark, Livy, Hive and Presto.\n",
    "- Mark the check boxes to use Glue Data Catalog on Spark, Hive and Presto.\n",
    "- Launch the EMR cluster with appropriate permissions set for **Systems Manager Session Manager** \n",
    "- Create a EMR Notebook attached to the created cluster and upload this notebook, and open it"
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DMS environment and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the DMS environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a Test Database and a DMS Full Load and CDC tasks to S3, executing this 2 previous labs:\n",
    "\n",
    "<strong> 1)  DMS Pre lab 1 (Create Test RDS Postgres)</strong> https://aws-dataengineering-day.workshop.aws/en/400/410-pre-lab-1.html\n",
    "\n",
    "<strong> 2) DMS Pre lab 2 (Create DMS environment) </strong> https://aws-dataengineering-day.workshop.aws/en/400/420-pre-lab-2.html\n",
    "\n",
    "<strong> 3) DMS Main Lab (Create and run Full Load Task) </strong> https://aws-dataengineering-day.workshop.aws/en/400/430-main-lab.html but instead on the step g. in the \"Create Target endpoint section\" https://aws-dataengineering-day.workshop.aws/en/400/430-main-lab.html#create-the-target-endpoint specify the following \"Extra connection attributes\" to generate Parquet data on S3:\n",
    "\n",
    "addColumnName=true;dataFormat=parquet;enableStatistics=true;encodingType=rle-dictionary;maxFileSize=524,288;parquetTimestampInMillisecond=true;parquetVersion=PARQUET_2_0;rfc4180=false;timestampColumnName=op_cdc_timestamp;\n",
    "\n",
    "<a href=\"https://ibb.co/wWyV3wd\"><img src=\"https://i.ibb.co/gTyNHPr/dms-taregt-Params.png\" alt=\"dms-taregt-Params\" border=\"0\"></a>\n",
    "\n",
    "\n",
    "<strong> 4) DMS Main Lab (CDC task for updates) </strong> execute the \"Create the CDC endpoint to replicate ongoing changes (Optional)\" part to enable a CDC task with the same Parquet endpoint used above:\n",
    "https://aws-dataengineering-day.workshop.aws/en/400/430-main-lab.html#create-the-cdc-endpoint-to-replicate-ongoing-changes-optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python AWS Wrangler and Psycopg lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aws wrangler is needed to submit the Spark Hudi job\n",
    "sc.install_pypi_package(\"awswrangler\") \n",
    "\n",
    "\n",
    "#sc.install_pypi_package(\"psycopg2\") //Optional to generate the updates from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Hudi Libraries on your EMR (5.30) cluster \n",
    "\n",
    "0. For the following steps to work, you should have launched the EMR cluster with appropriate permissions set for **Systems Manager Session Manager** \n",
    "1. From the AWS Console, type SSM in the search box and navigate to the **Amazon System Manager console**\n",
    "2. On the left hand side, select **Session Manager** from **Instances and Nodes** section\n",
    "3. Click on the start session and you should see two EC2 instances listed \n",
    "4. Select instance-id of the **EMR's Master** Node and click on **Start session**\n",
    "5. From the terminal type the following to change to user *ec2-user*\n",
    " \n",
    "```bash\n",
    "sh-4.2$ sudo su hadoop\n",
    "hadoop@ip-10-0-2-73 /]$ cd\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -ls /apps/hudi/lib/\n",
    "Found 4 items\n",
    "-rw-r--r--   1 hadoop hadoop     774384 2020-05-06 05:11 /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "-rw-r--r--   1 hadoop hadoop   20967361 2020-05-06 05:10 /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop   39051878 2020-05-06 05:10 /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop     187458 2020-05-06 05:10 /apps/hudi/lib/spark-avro.jar\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to write a Copy on Write (COW) table\n",
    "\n",
    "We will now run the DeltaStreamer utility as an EMR Step to write the above JSON formatted data into a Hudi dataset. To do that, we will need the following:\n",
    "\n",
    "* Properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source \n",
    "* Schema file for source dataset\n",
    "* Schema file for target dataset\n",
    "\n",
    "\n",
    "\n",
    "To run DeltStreamer Replace the following values in the below command in the text editor\n",
    "\n",
    "1. Set the cluster-id with the value from your EMR 5.3.0 cluster\n",
    "2. Replace xxxx part  with the S3 bucket name \n",
    "3. For -- target-base-path value with the S3 bucket name\n",
    "4. After replacing the values, execute the cell\n",
    "5. If the values are replaced correctly, you should see a step id displayed as the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "boto3.setup_default_session(region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the bulk insert job to create Copy on Write (COW) table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf468b70bb6433ebf0db84f6569f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf468b70bb6433ebf0db84f6569f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ab67b678204d179fd5874e94fe63c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-H8F2Q4SO16UCs-H8F2Q4SO16UC"
     ]
    }
   ],
   "source": [
    "#COW\n",
    "# SOURCE TABLE sporting_event_ticket\n",
    "\n",
    "cluster_id = \"j-XXXXXXX\"\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type COPY_ON_WRITE  \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field op_cdc_timestamp  \\\n",
    "--target-base-path s3://XXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket  \\\n",
    "--target-table hudi_sporting_event_ticket  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://XXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=id  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \\\n",
    "\"\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(step_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to create Merge on Read (MOR) table  -- (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae2c83e21114581890ababecfcc7e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae2c83e21114581890ababecfcc7e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-2U2MAIP88WEMs-2U2MAIP88WEM"
     ]
    }
   ],
   "source": [
    "#MOR\n",
    "# TABLE sporting_event\n",
    "\n",
    "cluster_id = \"j-2FBL7NHLI23IQ\"\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type MERGE_ON_READ  \\\n",
    "--op BULK_INSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field op_cdc_timestamp  \\\n",
    "--target-base-path s3://XXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket_mor  \\\n",
    "--target-table hudi_sporting_event_ticket_mor  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://XXXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=id  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket_mor   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(step_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the S3 path:\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<my bucket>/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/\n",
    "                           PRE .hoodie/\n",
    "2020-05-06 05:29:08          0 .hoodie_$folder$\n",
    "2020-05-06 05:29:28         93 .hoodie_partition_metadata\n",
    "2020-05-06 05:29:32    2317411 19cb0a3f-01f5-4590-a9de-df4edeb32125-0_4-4-36_20200506052917.parquet\n",
    "2020-05-06 05:29:30    2080361 2903b865-e188-4079-8e1a-3afeefe6543b-0_5-4-37_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2244907 3c454bf3-eb20-41d4-941b-cec82c10db6c-0_9-4-41_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2366016 440f2437-e94f-4580-88a8-7179b40e4400-0_2-4-34_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2305575 45b8bd53-7285-44cf-88c5-40aedb70e17d-0_0-4-32_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2234887 7cca038b-8e27-4ddd-8b30-521c85b4ba07-0_1-4-33_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2205694 9ede479c-3d7c-4b63-ba43-c9ba26ddb5bb-0_7-4-39_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2559101 b9af4f18-7663-4e73-ae3f-4b0703726f80-0_6-4-38_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2094234 c1bc770d-358c-48c5-8f47-f5b5a94e3867-0_8-4-40_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2246656 f671a969-6a6f-4759-a12d-c768dcac49f2-0_3-4-35_20200506052917.parquet\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait the step to finish and query the generated table on Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to <strong>Athena</strong>, and in the tables list (left panel) select the database <strong>\"default\"</strong>, you can now see the table \"hudi_sporting_event_ticket\" as part of the Glue Data Catalog\n",
    "\n",
    "2. Execute the following query to filter some example records and see the outputs.\n",
    "\n",
    "<ul>\n",
    "<p style=\"color:BLUE\">\n",
    "SELECT * FROM \"default\".\"hudi_sporting_event_ticket\" where id > 1 and id < 51;\n",
    "</p>\n",
    "    \n",
    "<a href=\"https://ibb.co/yRsSJ95\"><img src=\"https://i.ibb.co/g7gVC2w/athena-screenshot-2.png\" alt=\"athena-screenshot-2\" border=\"0\"></a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some CDC updates on the source Postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s generate some CDC data, by running updates now on the fake profile data generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1\n",
    "#### connect throught a SQL client from the source postgres\n",
    "\n",
    "Execute the following SQL statement (updates) on the source table <strong>sporting_event_ticket</strong>\n",
    "\n",
    "<p style=\"color:BLUE\">\n",
    "BEGIN;\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '1'; \n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '11'; \n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '21'; \n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '31';\n",
    "COMMIT; </br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2\n",
    "####  Use psycopg2 lib to execute remotely an Update query on the source RDS Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect an make an Update query on the source RDS Postgres\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "connection = psycopg2.connect(user = \"master\",\n",
    "                              password = \"master123\",\n",
    "                              host = \"XXXXXXXXXXXX.us-west-2.rds.amazonaws.com\",\n",
    "                              port = \"5432\",\n",
    "                              database = \"sportstickets\")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "# Print PostgreSQL Connection properties\n",
    "print ( connection.get_dsn_parameters(),\"\\n\")\n",
    "\n",
    "#Make some updates to the price to some existing records on the source RDS Postgres sporting_event_ticket table\n",
    "\n",
    "postgreSQL_select_Query = \"BEGIN;   \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '1';  \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '11'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '21'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '31'; \\\n",
    "COMMIT;\"\n",
    "\n",
    "cursor.execute(postgreSQL_select_Query)\n",
    "print(\"Executing update query\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a DeltaStreamer upsert job on COW Table to update the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a45f254fc7b4dbdb685677339f896b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a45f254fc7b4dbdb685677339f896b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfb20431dcb45be83fe2db84488376a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-SDJRP8DOGS7Ys-SDJRP8DOGS7Y"
     ]
    }
   ],
   "source": [
    "cluster_id = \"j-XXXXXXX\"\n",
    "\n",
    "# SOURCE TABLE sporting_event_ticket \n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type COPY_ON_WRITE  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--op UPSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field op_cdc_timestamp  \\\n",
    "--target-base-path s3://XXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket  \\\n",
    "--target-table hudi_sporting_event_ticket  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://XXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=id  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(step_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Notice the new records (files) on S3 </strong>\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://XXXXXXX-hudi-dms-cdc/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket\n",
    "2020-05-06 05:47:53    3686902 profile_1.json\n",
    "2020-05-06 05:48:40    3686509 profile_10.json\n",
    "2020-05-06 05:48:46    3682899 profile_11.json\n",
    "2020-05-06 05:48:51    3685701 profile_12.json\n",
    "2020-05-06 05:48:56    3682880 profile_13.json\n",
    "2020-05-06 05:49:02    3687458 profile_14.json\n",
    "2020-05-06 05:47:58    3686045 profile_2.json\n",
    "2020-05-06 05:48:03    3686088 profile_3.json\n",
    "2020-05-06 05:48:08    3687066 profile_4.json\n",
    "2020-05-06 05:48:14    3686211 profile_5.json\n",
    "2020-05-06 05:48:19    3685343 profile_6.json\n",
    "2020-05-06 05:48:24    3687729 profile_7.json\n",
    "2020-05-06 05:48:30    3685578 profile_8.json\n",
    "2020-05-06 05:48:35    3686606 profile_9.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the updated Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the <strong >S3 path of output location </strong> on any partition. Notice the new Parquet files. \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://xxxxxx-hudi-dms-cdc/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/A/\n",
    "\n",
    "2020-05-06 05:29:32    2317411 19cb0a3f-01f5-4590-a9de-df4edeb32125-0_4-4-36_20200506052917.parquet\n",
    "2020-05-06 06:32:48    2320387 19cb0a3f-01f5-4590-a9de-df4edeb32125-0_8-21-135_20200506063222.parquet\n",
    "2020-05-06 05:29:30    2080361 2903b865-e188-4079-8e1a-3afeefe6543b-0_5-4-37_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2085116 2903b865-e188-4079-8e1a-3afeefe6543b-0_7-21-134_20200506063222.parquet\n",
    "2020-05-06 06:32:47    2248999 3c454bf3-eb20-41d4-941b-cec82c10db6c-0_5-21-132_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2244907 3c454bf3-eb20-41d4-941b-cec82c10db6c-0_9-4-41_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2366016 440f2437-e94f-4580-88a8-7179b40e4400-0_2-4-34_20200506052917.parquet\n",
    "2020-05-06 06:32:48    2369677 440f2437-e94f-4580-88a8-7179b40e4400-0_9-21-136_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2305575 45b8bd53-7285-44cf-88c5-40aedb70e17d-0_0-4-32_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2310293 45b8bd53-7285-44cf-88c5-40aedb70e17d-0_4-21-131_20200506063222.parquet\n",
    "2020-05-06 05:29:31    2234887 7cca038b-8e27-4ddd-8b30-521c85b4ba07-0_1-4-33_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2238812 7cca038b-8e27-4ddd-8b30-521c85b4ba07-0_6-21-133_20200506063222.parquet\n",
    "2020-05-06 06:32:48    2209143 9ede479c-3d7c-4b63-ba43-c9ba26ddb5bb-0_2-21-129_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2205694 9ede479c-3d7c-4b63-ba43-c9ba26ddb5bb-0_7-4-39_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2563854 b9af4f18-7663-4e73-ae3f-4b0703726f80-0_1-21-128_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2559101 b9af4f18-7663-4e73-ae3f-4b0703726f80-0_6-4-38_20200506052917.parquet\n",
    "2020-05-06 06:32:48    2098070 c1bc770d-358c-48c5-8f47-f5b5a94e3867-0_3-21-130_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2094234 c1bc770d-358c-48c5-8f47-f5b5a94e3867-0_8-4-40_20200506052917.parquet\n",
    "2020-05-06 06:32:47    2249129 f671a969-6a6f-4759-a12d-c768dcac49f2-0_0-21-127_20200506063222.parquet\n",
    "2020-05-06 05:29:31    2246656 f671a969-6a6f-4759-a12d-c768dcac49f2-0_3-4-35_20200506052917.parquet\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run again the SQL statement on Athena to verify the updated records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Execute the same above query to filter some example records and verify the UPDATES.\n",
    "\n",
    "<ul>\n",
    "<p style=\"color:BLUE\">\n",
    "SELECT * FROM \"default\".\"hudi_sporting_event_ticket\" where id > 1 and id < 51;\n",
    "</p>\n",
    "\n",
    "<a href=\"https://ibb.co/yRsSJ95\"><img src=\"https://i.ibb.co/g7gVC2w/athena-screenshot-2.png\" alt=\"athena-screenshot-2\" border=\"0\"></a>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, check the Hudi CLI to list the different COMMITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check out Hudi CLI\n",
    "\n",
    "Execute on the Terminal: <strong> hudi-cli </strong> to access the HUDI CLI, and execute the following commands:\n",
    "\n",
    "<strong>hudi:</strong> connect --path s3://XXXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/\n",
    "\n",
    "<strong>hudi: hudi_sporting_event_ticket -> </strong> commits show\n",
    "\n",
    "See the different commits (you can rollback to a specific one)\n",
    "\n",
    "```\n",
    "20/05/06 06:47:14 INFO timeline.HoodieActiveTimeline: Loadedinstants java.util.stream.ReferencePipeline$Head@d689f6\n",
    "20/05/06 06:47:15 INFO s3n.S3NativeFileSystem: Opening 's3://XXXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/.hoodie/20200506063222.commit' for reading\n",
    "20/05/06 06:47:15 INFO s3n.S3NativeFileSystem: Opening 's3://XXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/.hoodie/20200506052917.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200506063222 │ 21.6 MB             │ 0                 │  4                  │ 1                        │ 150000                │ 140000                       │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200506052917 │ 21.6 MB             │ 10                │ 0                   │ 1                        │ 150000                │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
