{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with AWS DMS tasks and Apache Hudi Deltastreamer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Apache Hudi Deltastreamer\n",
    "HoodieDeltaStreamer utility is part of hudi-utilities-bundle that provides a way to ingest data from sources such as DFS or Kafka.\n",
    "\n",
    "In this notebook, you will learn to use DeltaStreamer Utility to bulk insert data into a Hudi Dataset as a Copy on Write(CoW) and Merge on Write (MOR) using a DMS Full load and CDC task as a source.  \n",
    "\n",
    "We will run queries in hudi-cli and SparkSQL to verify the tables and subsequent updates are incorporated into our datalake on Amazon S3\n",
    "\n",
    "Let's get started !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DMS environment and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the DMS environment to generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a Test Database and a DMS Full Load and CDC tasks to S3, executing this 2 previous labs:\n",
    "\n",
    "1) DMS Pre lab 1 (Create Test RDS Postgres) https://aws-dataengineering-day.workshop.aws/en/400/410-pre-lab-1.html\n",
    "\n",
    "2) DMS Pre lab 2 (Create DMS environment) https://aws-dataengineering-day.workshop.aws/en/400/420-pre-lab-2.html\n",
    "\n",
    "3) DMS Main Lab (Create and run Full Load Task) https://aws-dataengineering-day.workshop.aws/en/400/430-main-lab.html but instead on the step g. in the \"Create Target endpoint section\" https://aws-dataengineering-day.workshop.aws/en/400/430-main-lab.html#create-the-target-endpoint specify the following \"Extra connection attributes\" to generate Parquet data on S3:\n",
    "\n",
    "addColumnName=true;dataFormat=parquet;enableStatistics=true;encodingType=rle-dictionary;maxFileSize=524,288;parquetTimestampInMillisecond=true;parquetVersion=PARQUET_2_0;rfc4180=false;timestampColumnName=op_cdc_timestamp;\n",
    "\n",
    "<a href=\"https://ibb.co/wWyV3wd\"><img src=\"https://i.ibb.co/gTyNHPr/dms-taregt-Params.png\" alt=\"dms-taregt-Params\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Python AWS Wrangler and Psycopg lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29172d7924e490aa63057be49dcad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29172d7924e490aa63057be49dcad52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>11</td><td>application_1596580355664_0060</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-52-188.us-west-2.compute.internal:20888/proxy/application_1596580355664_0060/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-49-169.us-west-2.compute.internal:8042/node/containerlogs/container_1596580355664_0060_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>11</td><td>application_1596580355664_0060</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-52-188.us-west-2.compute.internal:20888/proxy/application_1596580355664_0060/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-49-169.us-west-2.compute.internal:8042/node/containerlogs/container_1596580355664_0060_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a26bd5cb3847ea960e95b2945e6116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/8f/1c5690eebf148d1d1554fc00ccf9101e134636553dbb75bdfef4f85d7647/psycopg2-2.8.5.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    /usr/lib64/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'project_urls'\n",
      "      warnings.warn(msg)\n",
      "    running egg_info\n",
      "    creating pip-egg-info/psycopg2.egg-info\n",
      "    writing pip-egg-info/psycopg2.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt\n",
      "    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'\n",
      "    warning: manifest_maker: standard file '-c' not found\n",
      "    \n",
      "    \n",
      "    Error: pg_config executable not found.\n",
      "    \n",
      "    pg_config is required to build psycopg2 from source.  Please add the directory\n",
      "    containing pg_config to the $PATH or specify the full executable path with the\n",
      "    option:\n",
      "    \n",
      "        python setup.py build_ext --pg-config /path/to/pg_config build ...\n",
      "    \n",
      "    or with the pg_config option in 'setup.cfg'.\n",
      "    \n",
      "    If you prefer to avoid building psycopg2 from source, please install the PyPI\n",
      "    'psycopg2-binary' package instead.\n",
      "    \n",
      "    For further information please check the 'doc/src/install.rst' file (also at\n",
      "    <https://www.psycopg.org/docs/install.html>).\n",
      "    \n",
      "    \n",
      "    ----------------------------------------\n",
      "\n",
      "Collecting awswrangler\n",
      "  Using cached https://files.pythonhosted.org/packages/13/e5/825d557fadca8462ef1c155cbd08f8d3fe907914f9d1ca38e9ea69348784/awswrangler-1.7.0-py3-none-any.whl\n",
      "Collecting SQLAlchemy~=1.3.10 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/0b/1e/2ecb43005ea73c650687071165901cc5b4dc966ede82b9fada0dc0a01e79/SQLAlchemy-1.3.18-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting boto3<2.0.0,>=1.12.49 (from awswrangler)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/5d/f55f8bd9dca9b758836603de7d6448e577b69730d3fccbbe8ada2d4eeaa1/boto3-1.14.37-py2.py3-none-any.whl (129kB)\n",
      "Collecting botocore<2.0.0,>=1.15.49 (from awswrangler)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/da/55bdf7fff2ce578d38817515acf135cde6226d1604d6876fe840d44c386d/botocore-1.17.37-py2.py3-none-any.whl (6.5MB)\n",
      "Collecting pandas<1.2.0,>=1.0.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/94/b1/f77f49cc7cc538b247f30c2ae7e3a50f29e44f0b1af32ff4869d7de3c762/pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting sqlalchemy-redshift<0.9.0,>=0.7.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/51/13b5422cfc08bee0d2aee1bb782fbc13e94fb204022838f371e513aa20dc/sqlalchemy_redshift-0.8.1-py2.py3-none-any.whl\n",
      "Collecting pymysql<0.11.0,>=0.9.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/57/af502e0e113f139b3f3add4f1efba899a730a365d2264d476e85b9591da5/PyMySQL-0.10.0-py2.py3-none-any.whl\n",
      "Collecting numpy<1.20.0,>=1.18.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/d1/90cd7e0b27ee86d77f5386d38b74520486100286d50772377791b6ef22ff/numpy-1.19.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting s3fs==0.4.2 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/e4/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675/s3fs-0.4.2-py3-none-any.whl\n",
      "Collecting pyarrow~=1.0.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/06/3fc0a3daa53e9a65592c1074e9b11f66d41ea8474a77d9f732d7abb63301/pyarrow-1.0.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting psycopg2-binary~=2.8.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/bc/cb407e8d0301801d5f44d3f464485d9577c3bf92db13afef4d05d757ec47/psycopg2_binary-2.8.5-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3<2.0.0,>=1.12.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler)\n",
      "Collecting urllib3<1.26,>=1.20; python_version != \"3.4\" (from botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas<1.2.0,>=1.0.0->awswrangler)\n",
      "Collecting packaging (from sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0 (from s3fs==0.4.2->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/66/974e01194980d9780cc09724315111f9cccba26b4351552fdb4d97eb842e/fsspec-0.8.0-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "Collecting pyparsing>=2.0.2 (from packaging->sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Installing collected packages: SQLAlchemy, urllib3, python-dateutil, docutils, botocore, s3transfer, boto3, numpy, pandas, pyparsing, packaging, sqlalchemy-redshift, pymysql, fsspec, s3fs, pyarrow, psycopg2-binary, awswrangler\n",
      "  Found existing installation: numpy 1.16.5\n",
      "    Not uninstalling numpy at /usr/local/lib64/python3.7/site-packages, outside environment /tmp/1596743843896-0\n",
      "Successfully installed SQLAlchemy-1.3.18 awswrangler-1.7.0 boto3-1.14.37 botocore-1.17.37 docutils-0.15.2 fsspec-0.8.0 numpy-1.19.1 packaging-20.4 pandas-1.1.0 psycopg2-binary-2.8.5 pyarrow-1.0.0 pymysql-0.10.0 pyparsing-2.4.7 python-dateutil-2.8.1 s3fs-0.4.2 s3transfer-0.3.3 sqlalchemy-redshift-0.8.1 urllib3-1.25.10\n",
      "\n",
      "Command \"python setup.py egg_info\" failed with error code 1 in /mnt/tmp/pip-build-72vc1f6i/psycopg2/Collecting psycopg2\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/8f/1c5690eebf148d1d1554fc00ccf9101e134636553dbb75bdfef4f85d7647/psycopg2-2.8.5.tar.gz\n",
      "    Complete output from command python setup.py egg_info:\n",
      "    /usr/lib64/python3.7/distutils/dist.py:274: UserWarning: Unknown distribution option: 'project_urls'\n",
      "      warnings.warn(msg)\n",
      "    running egg_info\n",
      "    creating pip-egg-info/psycopg2.egg-info\n",
      "    writing pip-egg-info/psycopg2.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt\n",
      "    writing manifest file 'pip-egg-info/psycopg2.egg-info/SOURCES.txt'\n",
      "    warning: manifest_maker: standard file '-c' not found\n",
      "    \n",
      "    \n",
      "    Error: pg_config executable not found.\n",
      "    \n",
      "    pg_config is required to build psycopg2 from source.  Please add the directory\n",
      "    containing pg_config to the $PATH or specify the full executable path with the\n",
      "    option:\n",
      "    \n",
      "        python setup.py build_ext --pg-config /path/to/pg_config build ...\n",
      "    \n",
      "    or with the pg_config option in 'setup.cfg'.\n",
      "    \n",
      "    If you prefer to avoid building psycopg2 from source, please install the PyPI\n",
      "    'psycopg2-binary' package instead.\n",
      "    \n",
      "    For further information please check the 'doc/src/install.rst' file (also at\n",
      "    <https://www.psycopg.org/docs/install.html>).\n",
      "    \n",
      "    \n",
      "    ----------------------------------------\n",
      "\n",
      "Collecting awswrangler\n",
      "  Using cached https://files.pythonhosted.org/packages/13/e5/825d557fadca8462ef1c155cbd08f8d3fe907914f9d1ca38e9ea69348784/awswrangler-1.7.0-py3-none-any.whl\n",
      "Collecting SQLAlchemy~=1.3.10 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/0b/1e/2ecb43005ea73c650687071165901cc5b4dc966ede82b9fada0dc0a01e79/SQLAlchemy-1.3.18-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting boto3<2.0.0,>=1.12.49 (from awswrangler)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/5d/f55f8bd9dca9b758836603de7d6448e577b69730d3fccbbe8ada2d4eeaa1/boto3-1.14.37-py2.py3-none-any.whl (129kB)\n",
      "Collecting botocore<2.0.0,>=1.15.49 (from awswrangler)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/da/55bdf7fff2ce578d38817515acf135cde6226d1604d6876fe840d44c386d/botocore-1.17.37-py2.py3-none-any.whl (6.5MB)\n",
      "Collecting pandas<1.2.0,>=1.0.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/94/b1/f77f49cc7cc538b247f30c2ae7e3a50f29e44f0b1af32ff4869d7de3c762/pandas-1.1.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting sqlalchemy-redshift<0.9.0,>=0.7.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/51/13b5422cfc08bee0d2aee1bb782fbc13e94fb204022838f371e513aa20dc/sqlalchemy_redshift-0.8.1-py2.py3-none-any.whl\n",
      "Collecting pymysql<0.11.0,>=0.9.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/57/af502e0e113f139b3f3add4f1efba899a730a365d2264d476e85b9591da5/PyMySQL-0.10.0-py2.py3-none-any.whl\n",
      "Collecting numpy<1.20.0,>=1.18.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/d1/90cd7e0b27ee86d77f5386d38b74520486100286d50772377791b6ef22ff/numpy-1.19.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting s3fs==0.4.2 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/b8/e4/b8fc59248399d2482b39340ec9be4bb2493846ac23641b43115a7e5cd675/s3fs-0.4.2-py3-none-any.whl\n",
      "Collecting pyarrow~=1.0.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/06/3fc0a3daa53e9a65592c1074e9b11f66d41ea8474a77d9f732d7abb63301/pyarrow-1.0.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting psycopg2-binary~=2.8.0 (from awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/e6/bc/cb407e8d0301801d5f44d3f464485d9577c3bf92db13afef4d05d757ec47/psycopg2_binary-2.8.5-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3<2.0.0,>=1.12.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler)\n",
      "Collecting urllib3<1.26,>=1.20; python_version != \"3.4\" (from botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting docutils<0.16,>=0.10 (from botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas<1.2.0,>=1.0.0->awswrangler)\n",
      "Collecting packaging (from sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0 (from s3fs==0.4.2->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/d3/66/974e01194980d9780cc09724315111f9cccba26b4351552fdb4d97eb842e/fsspec-0.8.0-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.15.49->awswrangler)\n",
      "Collecting pyparsing>=2.0.2 (from packaging->sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Installing collected packages: SQLAlchemy, urllib3, python-dateutil, docutils, botocore, s3transfer, boto3, numpy, pandas, pyparsing, packaging, sqlalchemy-redshift, pymysql, fsspec, s3fs, pyarrow, psycopg2-binary, awswrangler\n",
      "  Found existing installation: numpy 1.16.5\n",
      "    Not uninstalling numpy at /usr/local/lib64/python3.7/site-packages, outside environment /tmp/1596743843896-0\n",
      "Successfully installed SQLAlchemy-1.3.18 awswrangler-1.7.0 boto3-1.14.37 botocore-1.17.37 docutils-0.15.2 fsspec-0.8.0 numpy-1.19.1 packaging-20.4 pandas-1.1.0 psycopg2-binary-2.8.5 pyarrow-1.0.0 pymysql-0.10.0 pyparsing-2.4.7 python-dateutil-2.8.1 s3fs-0.4.2 s3transfer-0.3.3 sqlalchemy-redshift-0.8.1 urllib3-1.25.10\n",
      "\n",
      "Command \"python setup.py egg_info\" failed with error code 1 in /mnt/tmp/pip-build-72vc1f6i/psycopg2/"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"psycopg2\")\n",
    "sc.install_pypi_package(\"awswrangler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Hudi Libraries on your EMR (5.30) cluster \n",
    "\n",
    "0. For the following steps to work, you should have launched the EMR cluster with appropriate permissions set for **Systems Manager Session Manager** \n",
    "1. From the AWS Console, type SSM in the search box and navigate to the **Amazon System Manager console**\n",
    "2. On the left hand side, select **Session Manager** from **Instances and Nodes** section\n",
    "3. Click on the start session and you should see two EC2 instances listed \n",
    "4. Select instance-id of the **EMR's Master** Node and click on **Start session**\n",
    "5. From the terminal type the following to change to user *ec2-user*\n",
    " \n",
    "```bash\n",
    "sh-4.2$ sudo su hadoop\n",
    "hadoop@ip-10-0-2-73 /]$ cd\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-utilities-bundle.jar /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "hadoop@ip-10-0-2-73 ~]$ hdfs dfs -copyFromLocal /usr/lib/spark/jars/httpclient-4.5.9.jar /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "[hadoop@ip-10-0-2-73 ~]$ hdfs dfs -ls /apps/hudi/lib/\n",
    "Found 4 items\n",
    "-rw-r--r--   1 hadoop hadoop     774384 2020-05-06 05:11 /apps/hudi/lib/httpclient-4.5.9.jar\n",
    "-rw-r--r--   1 hadoop hadoop   20967361 2020-05-06 05:10 /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop   39051878 2020-05-06 05:10 /apps/hudi/lib/hudi-utilities-bundle.jar\n",
    "-rw-r--r--   1 hadoop hadoop     187458 2020-05-06 05:10 /apps/hudi/lib/spark-avro.jar\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DeltaStreamer to write a Copy on Write (COW) table\n",
    "\n",
    "We will now run the DeltaStreamer utility as an EMR Step to write the above JSON formatted data into a Hudi dataset. To do that, we will need the following:\n",
    "\n",
    "* Properties file on localfs or dfs, with configurations for Hudi client, schema provider, key generator and data source \n",
    "* Schema file for source dataset\n",
    "* Schema file for target dataset\n",
    "\n",
    "\n",
    "\n",
    "To run DeltStreamer Replace the following values in the below command in the text editor\n",
    "\n",
    "1. Set the cluster-id with the value from your EMR 5.3.0 cluster\n",
    "2. Replace xxxx part  with the S3 bucket name \n",
    "3. For -- target-base-path value with the S3 bucket name\n",
    "4. After replacing the values, execute the cell\n",
    "5. If the values are replaced correctly, you should see a step id displayed as the output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "boto3.setup_default_session(region_name=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the bulk insert job to create COW table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf468b70bb6433ebf0db84f6569f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf468b70bb6433ebf0db84f6569f718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ab67b678204d179fd5874e94fe63c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-H8F2Q4SO16UCs-H8F2Q4SO16UC"
     ]
    }
   ],
   "source": [
    "#COW\n",
    "# SOURCE TABLE sporting_event_ticket\n",
    "\n",
    "cluster_id = \"j-XXXXXXX\"\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type COPY_ON_WRITE  \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field op_cdc_timestamp  \\\n",
    "--target-base-path s3://XXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket  \\\n",
    "--target-table hudi_sporting_event_ticket  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://XXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=id  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor  \\\n",
    "\"\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(step_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to create Merge on Read (MOR) table -- (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae2c83e21114581890ababecfcc7e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae2c83e21114581890ababecfcc7e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-2U2MAIP88WEMs-2U2MAIP88WEM"
     ]
    }
   ],
   "source": [
    "#MOR\n",
    "# TABLE sporting_event\n",
    "\n",
    "cluster_id = \"j-2FBL7NHLI23IQ\"\n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type MERGE_ON_READ  \\\n",
    "--op BULK_INSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field op_cdc_timestamp  \\\n",
    "--target-base-path s3://XXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket_mor  \\\n",
    "--target-table hudi_sporting_event_ticket_mor  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://XXXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=id  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket_mor   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(step_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check the S3 path:\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<my bucket>/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket/\n",
    "                           PRE .hoodie/\n",
    "2020-05-06 05:29:08          0 .hoodie_$folder$\n",
    "2020-05-06 05:29:28         93 .hoodie_partition_metadata\n",
    "2020-05-06 05:29:32    2317411 19cb0a3f-01f5-4590-a9de-df4edeb32125-0_4-4-36_20200506052917.parquet\n",
    "2020-05-06 05:29:30    2080361 2903b865-e188-4079-8e1a-3afeefe6543b-0_5-4-37_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2244907 3c454bf3-eb20-41d4-941b-cec82c10db6c-0_9-4-41_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2366016 440f2437-e94f-4580-88a8-7179b40e4400-0_2-4-34_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2305575 45b8bd53-7285-44cf-88c5-40aedb70e17d-0_0-4-32_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2234887 7cca038b-8e27-4ddd-8b30-521c85b4ba07-0_1-4-33_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2205694 9ede479c-3d7c-4b63-ba43-c9ba26ddb5bb-0_7-4-39_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2559101 b9af4f18-7663-4e73-ae3f-4b0703726f80-0_6-4-38_20200506052917.parquet\n",
    "2020-05-06 05:29:32    2094234 c1bc770d-358c-48c5-8f47-f5b5a94e3867-0_8-4-40_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2246656 f671a969-6a6f-4759-a12d-c768dcac49f2-0_3-4-35_20200506052917.parquet\n",
    "```\n",
    "\n",
    "To query the Hudi dataset you can do one of the following\n",
    "\n",
    "1. - GO to Athena on AWS Console and look for 'default' database and the table'hudi_sport_tickets_events' ",
    "\n",
    "2. - Navigate to the another sparkmagic notebook and run queries in Spark using SparkMagic cell\n",
    "- SSH to the master node (you can also SSM if you launched your cluster with SSM permissions) and run queries using Hive/Presto\n",
    "- Head to the Hue console on Amazon EMR and run queries\n",
    "\n",
    "Let us use SparkSQL for instance. Login to your EMR cluster and start SparkSQL. \n",
    "\n",
    "```\n",
    "\n",
    "$ spark-sql --jars hdfs:///apps/lib/hudi/*.jar\n",
    "\n",
    "spark-sql> select * from profile_cow limit 2;\n",
    "\n",
    "20200506052917\t20200506052917_6_1\t993739fd-97af-494e-8920-267b4796a686\t\tb9af4f18-7663-4e73-ae3f-4b0703726f80-0_6-4-38_20200506052917.parquet\tStacy Lester\t142-413-1721x450\tAdvertising art director\tHarris Group\t426-69-1109\tUSS May\n",
    "FPO AE 08308\t1921-08-21\tcoxrachel@gmail.com\t2011-11-07T06:10:11\n",
    "20200506052917\t20200506052917_6_2\t993776e5-d124-4c12-8835-e322690fcfea\t\tb9af4f18-7663-4e73-ae3f-4b0703726f80-0_6-4-38_20200506052917.parquet\tKayla Boone\t(705)940-6720x57967\tPhysiological scientist\tSalazar-Alexander\t882-54-5400\t4702 Steven Ports\n",
    "Tracystad, CT 97143\t1972-05-18\tkristie74@ward.com\t2017-01-05T11:50:44\n",
    "Time taken: 9.263 seconds, Fetched 2 row(s)\n",
    "\n",
    "Now, lets make a note of street_address in one of these two records.\n",
    "\n",
    "spark-sql> select _hoodie_commit_time, street_address from profile_cow where _hoodie_record_key=\"993739fd-97af-494e-8920-267b4796a686\";\n",
    "\n",
    "20200506052917\tUSS May FPO AE 08308     \n",
    "\n",
    "Time taken: 3.86 seconds, Fetched 1 row(s)\n",
    "\n",
    "```\n",
    "\n",
    "Lets now run an upsert to observe the change in records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some updates on the source Postgres database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run updates now on the fake profile data generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect an make an Update query on the source RDS Postgres\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "connection = psycopg2.connect(user = \"master\",\n",
    "                              password = \"master123\",\n",
    "                              host = \"XXXXXXXXXXXX.us-west-2.rds.amazonaws.com\",\n",
    "                              port = \"5432\",\n",
    "                              database = \"sportstickets\")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "# Print PostgreSQL Connection properties\n",
    "print ( connection.get_dsn_parameters(),\"\\n\")\n",
    "\n",
    "#Make some updates to the price to some existing records on the source RDS Postgres sporting_event_ticket table\n",
    "\n",
    "postgreSQL_select_Query = \"BEGIN;   \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '1';  \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '11'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '21'; \\\n",
    "UPDATE dms_sample.sporting_event_ticket SET ticket_price = '111.99' WHERE  id = '31'; \\\n",
    "COMMIT;\"\n",
    "\n",
    "cursor.execute(postgreSQL_select_Query)\n",
    "print(\"Executing update query\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start DeltaStreamer upsert job on COW Table to update the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a45f254fc7b4dbdb685677339f896b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a45f254fc7b4dbdb685677339f896b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfb20431dcb45be83fe2db84488376a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s-SDJRP8DOGS7Ys-SDJRP8DOGS7Y"
     ]
    }
   ],
   "source": [
    "cluster_id = \"j-XXXXXXX\"\n",
    "\n",
    "# SOURCE TABLE sporting_event_ticket \n",
    "\n",
    "command= f\"spark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\n",
    "--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar  \\\n",
    "--master yarn  \\\n",
    "file:///usr/lib/hudi/hudi-utilities-bundle_2.11-0.5.2-incubating.jar  \\\n",
    "--table-type COPY_ON_WRITE  \\\n",
    "--payload-class org.apache.hudi.payload.AWSDmsAvroPayload  \\\n",
    "--op UPSERT \\\n",
    "--source-class org.apache.hudi.utilities.sources.ParquetDFSSource  \\\n",
    "--source-ordering-field op_cdc_timestamp  \\\n",
    "--target-base-path s3://XXXXXX/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket  \\\n",
    "--target-table hudi_sporting_event_ticket  \\\n",
    "--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer  \\\n",
    "--enable-hive-sync  \\\n",
    "--hoodie-conf hoodie.deltastreamer.source.dfs.root=s3://XXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.write.recordkey.field=id  \\\n",
    "--hoodie-conf hoodie.datasource.write.partitionpath.field=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_fields=seat_row \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.database=default  \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.table=hudi_sporting_event_ticket   \\\n",
    "--hoodie-conf hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "\n",
    "#Submit the job to the specified cluster (step)\n",
    "step_id = wr.emr.submit_step(cluster_id, command)\n",
    "print(step_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the records in updates/ location.\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://XXXXXXX-hudi-dms-cdc/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket\n",
    "2020-05-06 05:47:53    3686902 profile_1.json\n",
    "2020-05-06 05:48:40    3686509 profile_10.json\n",
    "2020-05-06 05:48:46    3682899 profile_11.json\n",
    "2020-05-06 05:48:51    3685701 profile_12.json\n",
    "2020-05-06 05:48:56    3682880 profile_13.json\n",
    "2020-05-06 05:49:02    3687458 profile_14.json\n",
    "2020-05-06 05:47:58    3686045 profile_2.json\n",
    "2020-05-06 05:48:03    3686088 profile_3.json\n",
    "2020-05-06 05:48:08    3687066 profile_4.json\n",
    "2020-05-06 05:48:14    3686211 profile_5.json\n",
    "2020-05-06 05:48:19    3685343 profile_6.json\n",
    "2020-05-06 05:48:24    3687729 profile_7.json\n",
    "2020-05-06 05:48:30    3685578 profile_8.json\n",
    "2020-05-06 05:48:35    3686606 profile_9.json\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the updated Hudi Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the S3 path of output location. Notice the new Parquet files. \n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://558139421422-hudi-dms-cdc/landing-hudi/co/tickets/dms_sample/hudi_sporting_event_ticket\n",
    "                           PRE .hoodie/\n",
    "2020-05-06 05:29:08          0 .hoodie_$folder$\n",
    "2020-05-06 05:29:28         93 .hoodie_partition_metadata\n",
    "2020-05-06 05:29:32    2317411 19cb0a3f-01f5-4590-a9de-df4edeb32125-0_4-4-36_20200506052917.parquet\n",
    "2020-05-06 06:32:48    2320387 19cb0a3f-01f5-4590-a9de-df4edeb32125-0_8-21-135_20200506063222.parquet\n",
    "2020-05-06 05:29:30    2080361 2903b865-e188-4079-8e1a-3afeefe6543b-0_5-4-37_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2085116 2903b865-e188-4079-8e1a-3afeefe6543b-0_7-21-134_20200506063222.parquet\n",
    "2020-05-06 06:32:47    2248999 3c454bf3-eb20-41d4-941b-cec82c10db6c-0_5-21-132_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2244907 3c454bf3-eb20-41d4-941b-cec82c10db6c-0_9-4-41_20200506052917.parquet\n",
    "2020-05-06 05:29:31    2366016 440f2437-e94f-4580-88a8-7179b40e4400-0_2-4-34_20200506052917.parquet\n",
    "2020-05-06 06:32:48    2369677 440f2437-e94f-4580-88a8-7179b40e4400-0_9-21-136_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2305575 45b8bd53-7285-44cf-88c5-40aedb70e17d-0_0-4-32_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2310293 45b8bd53-7285-44cf-88c5-40aedb70e17d-0_4-21-131_20200506063222.parquet\n",
    "2020-05-06 05:29:31    2234887 7cca038b-8e27-4ddd-8b30-521c85b4ba07-0_1-4-33_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2238812 7cca038b-8e27-4ddd-8b30-521c85b4ba07-0_6-21-133_20200506063222.parquet\n",
    "2020-05-06 06:32:48    2209143 9ede479c-3d7c-4b63-ba43-c9ba26ddb5bb-0_2-21-129_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2205694 9ede479c-3d7c-4b63-ba43-c9ba26ddb5bb-0_7-4-39_20200506052917.parquet\n",
    "2020-05-06 06:32:49    2563854 b9af4f18-7663-4e73-ae3f-4b0703726f80-0_1-21-128_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2559101 b9af4f18-7663-4e73-ae3f-4b0703726f80-0_6-4-38_20200506052917.parquet\n",
    "2020-05-06 06:32:48    2098070 c1bc770d-358c-48c5-8f47-f5b5a94e3867-0_3-21-130_20200506063222.parquet\n",
    "2020-05-06 05:29:32    2094234 c1bc770d-358c-48c5-8f47-f5b5a94e3867-0_8-4-40_20200506052917.parquet\n",
    "2020-05-06 06:32:47    2249129 f671a969-6a6f-4759-a12d-c768dcac49f2-0_0-21-127_20200506063222.parquet\n",
    "2020-05-06 05:29:31    2246656 f671a969-6a6f-4759-a12d-c768dcac49f2-0_3-4-35_20200506052917.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Let's query an upserted record. \n",
    "\n",
    "```\n",
    "$ spark-sql --jars hdfs:///apps/lib/hudi/*.jar \n",
    "\n",
    "spark-sql> select _hoodie_commit_time, street_address from profile_cow where _hoodie_record_key=\"993739fd-97af-494e-8920-267b4796a686\";\n",
    "\n",
    "20200506052917\tUSS May FPO AE 08308                                     # Old address \n",
    "20200506063222\t880 Jackson Mountains Suite 221 Lake Kristin, SC 39264   # Our recent update \n",
    "\n",
    "Time taken: 7.901 seconds, Fetched 2 row(s)\n",
    "\n",
    "```\n",
    "\n",
    "Now lets check out Hudi CLI\n",
    "\n",
    "```\n",
    "\n",
    "hudi:person_profile_cow->commits show\n",
    "20/05/06 06:47:14 INFO timeline.HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@d689f6\n",
    "20/05/06 06:47:15 INFO s3n.S3NativeFileSystem: Opening 's3://XXXXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket/.hoodie/20200506063222.commit' for reading\n",
    "20/05/06 06:47:15 INFO s3n.S3NativeFileSystem: Opening 's3://XXXXX/landing-hudi/co/tickets/dms_sample/sporting_event_ticket/.hoodie/20200506052917.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200506063222 │ 21.6 MB             │ 0                 │ 10                  │ 1                        │ 150000                │ 140000                       │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200506052917 │ 21.6 MB             │ 10                │ 0                   │ 1                        │ 150000                │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
