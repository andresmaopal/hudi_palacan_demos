{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo : Batch Updates to a S3 Datalake using Apache Hudi\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Overview](#Overview)\n",
    "2. [Copy On Write](#Copy-On-Write)\n",
    "   2.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "   2.2 [Batch Upsert some records](#Batch-Upsert-some-records)\n",
    "   2.3 [Deleting Records](#Deleting-Records.)\n",
    "3. [Rollback](#Rollback)\n",
    "4. [Time travel with Hudi](#Time-travel-with-Hudi) \n",
    "5. [Merge on Read](#Merge-On-Read) \n",
    "   5.1 [Bulk Insert the Initial Dataset](#Bulk-Insert-the-Initial-Dataset)\n",
    "   5.2 [Batch Upsert some records](#Batch-Upsert-some-records)\n",
    "   5.3 [Compaction for MOR tables](#Compaction-for-MOR-tables) \n",
    "6. [Working with Partitioned Tables](#Working-with-Partitioned-Tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates using PySpark on [Apache Hudi](https://aws.amazon.com/emr/features/hudi/) on Amazon EMR to insert/upsert/delete records to an S3 data lake.\n",
    "\n",
    "Here are some good reference links to read later:\n",
    "\n",
    "* [Apache Hudi concepts](https://hudi.apache.org/concepts.html)\n",
    "* [How Hudi Works](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-how-it-works.html)\n",
    "\n",
    "This notebook covers the following concepts when writing Copy-On-Write and Merge-On-Read tables to an S3 Datalake:\n",
    "\n",
    "- Write Hudi Spark jobs in PySpark.\n",
    "- Bulk Insert the Initial Dataset.\n",
    "- Write a MultiKey Partitioned table as well as a Non-Partitioned table.\n",
    "- Tune the Bulk Insert write performance as per expected number of target files.\n",
    "- Sync the Hudi tables to the Hive/Glue Catalog.\n",
    "- Upsert some records to a Hudi table.\n",
    "- Delete from records from a Hudi table.\n",
    "- Understand how Hudi Commit Retention policy works.\n",
    "- Time travel with Hudi incremental tables using incremental and point-in-time queries\n",
    "- Perform Insert/Upsert/Delete operations for Merge-on-Read table and understand the difference between MOR and COW tables\n",
    "\n",
    "\n",
    "#### This demo runs fine on an EMR version 6.1 Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 'hdfs:///apps/hudi/lib/httpclient-4.5.9.jar, hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.dynamicAllocation.executorIdleTimeout': 3600, 'spark.executor.memory': '7G', 'spark.executor.cores': 3, 'spark.dynamicAllocation.initialExecutors': 16}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":  { \n",
    "             \"spark.jars\":\"hdfs:///apps/hudi/lib/httpclient-4.5.9.jar, hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "             \"spark.sql.hive.convertMetastoreParquet\":\"false\",     \n",
    "             \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "             \"spark.dynamicAllocation.executorIdleTimeout\": 3600,\n",
    "             \"spark.executor.memory\": \"7G\",\n",
    "             \"spark.executor.cores\": 3,\n",
    "             \"spark.dynamicAllocation.initialExecutors\":16\n",
    "           } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a17ca133f604a169731488614caa6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1604957251481_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-35-204.us-west-2.compute.internal:20888/proxy/application_1604957251481_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-32-64.us-west-2.compute.internal:8042/node/containerlogs/container_1604957251481_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "## CHANGE ME ##\n",
    "\n",
    "S3_bucket = \"hudi-dms-repl-dmslabs3bucket-XXXXXXX\"\n",
    "S3_target_prefix = \"workshop1/hudi_trips_table\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"table_name\": \"hudi_trips_table\",\n",
    "    \"target\": (\"s3://{}/{}\".format(S3_bucket,S3_target_prefix)),\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants for Python to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910ef4141d9f4db5bc7408687186f18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General Constants\n",
    "HUDI_FORMAT = \"org.apache.hudi\"\n",
    "TABLE_NAME = \"hoodie.table.name\"\n",
    "RECORDKEY_FIELD_OPT_KEY = \"hoodie.datasource.write.recordkey.field\"\n",
    "PRECOMBINE_FIELD_OPT_KEY = \"hoodie.datasource.write.precombine.field\"\n",
    "OPERATION_OPT_KEY = \"hoodie.datasource.write.operation\"\n",
    "BULK_INSERT_OPERATION_OPT_VAL = \"bulk_insert\"\n",
    "UPSERT_OPERATION_OPT_VAL = \"upsert\"\n",
    "BULK_INSERT_PARALLELISM = \"hoodie.bulkinsert.shuffle.parallelism\" \n",
    "UPSERT_PARALLELISM = \"hoodie.upsert.shuffle.parallelism\"\n",
    "S3_CONSISTENCY_CHECK = \"hoodie.consistency.check.enabled\"\n",
    "HUDI_CLEANER_POLICY = \"hoodie.cleaner.policy\"\n",
    "KEEP_LATEST_COMMITS = \"KEEP_LATEST_COMMITS\"\n",
    "HUDI_COMMITS_RETAINED = \"hoodie.cleaner.commits.retained\"\n",
    "PAYLOAD_CLASS_OPT_KEY = \"hoodie.datasource.write.payload.class\"\n",
    "EMPTY_PAYLOAD_CLASS_OPT_VAL = \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\"\n",
    "\n",
    "# Hive Constants\n",
    "HIVE_SYNC_ENABLED_OPT_KEY=\"hoodie.datasource.hive_sync.enable\"\n",
    "HIVE_PARTITION_FIELDS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_fields\"\n",
    "HIVE_ASSUME_DATE_PARTITION_OPT_KEY=\"hoodie.datasource.hive_sync.assume_date_partitioning\"\n",
    "HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY=\"hoodie.datasource.hive_sync.partition_extractor_class\"\n",
    "HIVE_TABLE_OPT_KEY=\"hoodie.datasource.hive_sync.table\"\n",
    "\n",
    "# Partition Constants\n",
    "NONPARTITION_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.NonPartitionedExtractor\"\n",
    "MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL=\"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "KEYGENERATOR_CLASS_OPT_KEY=\"hoodie.datasource.write.keygenerator.class\"\n",
    "NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.keygen.NonpartitionedKeyGenerator\"\n",
    "COMPLEX_KEYGENERATOR_CLASS_OPT_VAL=\"org.apache.hudi.ComplexKeyGenerator\"\n",
    "PARTITIONPATH_FIELD_OPT_KEY=\"hoodie.datasource.write.partitionpath.field\"\n",
    "\n",
    "#Incremental Constants\n",
    "VIEW_TYPE_OPT_KEY=\"hoodie.datasource.view.type\"\n",
    "BEGIN_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.begin.instanttime\"\n",
    "VIEW_TYPE_INCREMENTAL_OPT_VAL=\"incremental\"\n",
    "END_INSTANTTIME_OPT_KEY=\"hoodie.datasource.read.end.instanttime\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to create JSON data and Spark dataframe from this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43055d25dc548ff9294507d2db2abdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generates Data\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_data(start, count, dest):\n",
    "    time_stamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    data = [{\"trip_id\": i, \"tstamp\": time_stamp, \"route_id\": chr(65 + (i % 10)), \"destination\": dest[i%10]} for i in range(start, start + count)]\n",
    "    return data\n",
    "\n",
    "# Creates the Dataframe\n",
    "def create_json_df(spark, data):\n",
    "    sc = spark.sparkContext\n",
    "    return spark.read.json(sc.parallelize(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulk Insert the Initial Dataset\n",
    "\n",
    "Let's generate 2M records to load into our Data Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d8e599bdd74d1781501b04fa7f7d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-----------------+--------+-------+-------------------+\n",
      "|      destination|route_id|trip_id|             tstamp|\n",
      "+-----------------+--------+-------+-------------------+\n",
      "|       Bogotá D.C|       A|      0|2020-11-09 23:31:09|\n",
      "|             Lima|       B|      1|2020-11-09 23:31:09|\n",
      "| Ciudad de México|       C|      2|2020-11-09 23:31:09|\n",
      "|         Medellín|       D|      3|2020-11-09 23:31:09|\n",
      "|        Cartagena|       E|      4|2020-11-09 23:31:09|\n",
      "|Santiago de Chile|       F|      5|2020-11-09 23:31:09|\n",
      "|        Sao Paulo|       G|      6|2020-11-09 23:31:09|\n",
      "|   Rio de Janeiro|       H|      7|2020-11-09 23:31:09|\n",
      "|            Quito|       I|      8|2020-11-09 23:31:09|\n",
      "|          Córdoba|       J|      9|2020-11-09 23:31:09|\n",
      "|       Bogotá D.C|       A|     10|2020-11-09 23:31:09|\n",
      "|             Lima|       B|     11|2020-11-09 23:31:09|\n",
      "| Ciudad de México|       C|     12|2020-11-09 23:31:09|\n",
      "|         Medellín|       D|     13|2020-11-09 23:31:09|\n",
      "|        Cartagena|       E|     14|2020-11-09 23:31:09|\n",
      "|Santiago de Chile|       F|     15|2020-11-09 23:31:09|\n",
      "|        Sao Paulo|       G|     16|2020-11-09 23:31:09|\n",
      "|   Rio de Janeiro|       H|     17|2020-11-09 23:31:09|\n",
      "|            Quito|       I|     18|2020-11-09 23:31:09|\n",
      "|          Córdoba|       J|     19|2020-11-09 23:31:09|\n",
      "+-----------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "dest = [\"Bogotá D.C\", \"Lima\", \"Ciudad de México\", \"Medellín\", \"Cartagena\", \"Santiago de Chile\",\"Sao Paulo\",\"Rio de Janeiro\",\"Quito\",\"Córdoba\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And write the data to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac57c5bee454f4d8410ddfd4147b012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the number of files in S3. Expected number of files is 3 files as BULK_INSERT_PARALLELISM is set to 3. \n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/data_source_demo/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 5\n",
    "   Total Size: 13.8 MiB\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "Let's inspect the table created and query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a635c9f3ec314718a53d3523de3de63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----------+\n",
      "|database|tableName       |isTemporary|\n",
      "+--------+----------------+-----------+\n",
      "|default |hudi_trips_table|false      |\n",
      "+--------+----------------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the extra columns that are added by Hudi to keep track of commits and filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a59baada734bcf96038fe04209b940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------------------------------------------------------+----------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                   |destination     |route_id|trip_id|tstamp             |\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------------------------------------------------------+----------------+--------+-------+-------------------+\n",
      "|20201109233125     |20201109233125_0_1  |0                 |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |0      |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_3  |1                 |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Lima            |B       |1      |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_5  |10                |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |10     |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_7  |100               |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |100    |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_9  |1000              |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |1000   |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_11 |10000             |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |10000  |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_13 |100000            |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |100000 |2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_14 |1000000           |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Bogotá D.C      |A       |1000000|2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_16 |1000001           |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Lima            |B       |1000001|2020-11-09 23:31:09|\n",
      "|20201109233125     |20201109233125_0_18 |1000002           |                      |b4e750db-5360-4479-aa97-d45c18060faf-0_0-8-27_20201109233125.parquet|Ciudad de México|C       |1000002|2020-11-09 23:31:09|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------------------------------------------------------+----------------+--------+-------+-------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "print(df2.count())\n",
    "df2.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the Hive table as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f10e2bdf0284cb999a266fdcb37f3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|2000000 |\n",
      "+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Upsert some records\n",
    "\n",
    "Let's modify a few records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b71dea20554ba785ddba398c745098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------+-------------------+\n",
      "|trip_id|route_id|destination      |tstamp             |\n",
      "+-------+--------+-----------------+-------------------+\n",
      "|1000000|A       |Bogotá D.C       |2020-11-09 23:31:09|\n",
      "|1000001|B       |Lima             |2020-11-09 23:31:09|\n",
      "|1000002|C       |Ciudad de México |2020-11-09 23:31:09|\n",
      "|1000003|D       |Medellín         |2020-11-09 23:31:09|\n",
      "|1000004|E       |Cartagena        |2020-11-09 23:31:09|\n",
      "|1000005|F       |Santiago de Chile|2020-11-09 23:31:09|\n",
      "|1000006|G       |Sao Paulo        |2020-11-09 23:31:09|\n",
      "|1000007|H       |Rio de Janeiro   |2020-11-09 23:31:09|\n",
      "|1000008|I       |Quito            |2020-11-09 23:31:09|\n",
      "|1000009|J       |Córdoba          |2020-11-09 23:31:09|\n",
      "+-------+--------+-----------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name'] +\" where trip_id between 1000000 and 1000009\").show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47b2c7aa8fe44e1ab9359475c6b7adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"Buenos Aires\", \"Buenos Aires\", \"Buenos Aires\", \"Buenos Aires\", \"Buenos Aires\",\"Buenos Aires\",\"Buenos Aires\",\"Buenos Aires\",\"Buenos Aires\",\"Buenos Aires\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584a12a6bbd34a6789f08072e980f089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+------------+\n",
      "|trip_id|route_id|             tstamp| destination|\n",
      "+-------+--------+-------------------+------------+\n",
      "|1000000|       A|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000001|       B|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000002|       C|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000003|       D|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000004|       E|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000005|       F|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000006|       G|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000007|       H|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000008|       I|2020-11-09 23:39:15|Buenos Aires|\n",
      "|1000009|       J|2020-11-09 23:39:15|Buenos Aires|\n",
      "+-------+--------+-------------------+------------+"
     ]
    }
   ],
   "source": [
    "df3.select(\"trip_id\",\"route_id\",\"tstamp\",\"destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have changed the destination and timestamp for trip IDs 1000000 to 1000010. Now, let's upsert the changes to S3. Note that the operation now is \"Upsert\" as opposed to BulkInsert for the initial load:\n",
    "\n",
    "```\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3f82eeb8f945ada33e804442e12d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683d2ac1e8124ab18142f121b794b42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+----------------+\n",
      "|trip_id|route_id|tstamp             |destination     |\n",
      "+-------+--------+-------------------+----------------+\n",
      "|1000000|A       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000001|B       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000002|C       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000003|D       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000004|E       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000005|F       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000006|G       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000007|H       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000008|I       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000009|J       |2020-11-09 23:39:15|Buenos Aires    |\n",
      "|1000010|A       |2020-11-09 23:31:09|Bogotá D.C      |\n",
      "|1000011|B       |2020-11-09 23:31:09|Lima            |\n",
      "|1000012|C       |2020-11-09 23:31:09|Ciudad de México|\n",
      "|1000013|D       |2020-11-09 23:31:09|Medellín        |\n",
      "|999996 |G       |2020-11-09 23:31:09|Sao Paulo       |\n",
      "|999997 |H       |2020-11-09 23:31:09|Rio de Janeiro  |\n",
      "|999998 |I       |2020-11-09 23:31:09|Quito           |\n",
      "|999999 |J       |2020-11-09 23:31:09|Córdoba         |\n",
      "+-------+--------+-------------------+----------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id between 999996 and 1000013\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check the commit\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 6\n",
    "   Total Size: 18.6 MiB\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we upserted some records, let us try to insert 10 new records into the table. We will use same upsert option. As primary keys 2000000 to 2000009 do not exist in the table, the records will be inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78bdca0ba3d448a8c83859da7a65214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|     La Paz|       A|2000000|2020-11-09 23:41:57|\n",
      "|     La Paz|       B|2000001|2020-11-09 23:41:57|\n",
      "|     La Paz|       C|2000002|2020-11-09 23:41:57|\n",
      "|     La Paz|       D|2000003|2020-11-09 23:41:57|\n",
      "|     La Paz|       E|2000004|2020-11-09 23:41:57|\n",
      "|     La Paz|       F|2000005|2020-11-09 23:41:57|\n",
      "|     La Paz|       G|2000006|2020-11-09 23:41:57|\n",
      "|     La Paz|       H|2000007|2020-11-09 23:41:57|\n",
      "|     La Paz|       I|2000008|2020-11-09 23:41:57|\n",
      "|     La Paz|       J|2000009|2020-11-09 23:41:57|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "insert_dest = [\"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\", \"La Paz\"]\n",
    "df5 = create_json_df(spark, get_json_data(2000000, 10, insert_dest))\n",
    "df5.count()\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9772d449be1a49e78dff810ac82fc416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366dedfd6cd74ebb9cd469ef381c5064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000010"
     ]
    }
   ],
   "source": [
    "df6=spark.read.format(HUDI_FORMAT).load(config[\"target\"]+\"/*\")\n",
    "df6.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbcb2f6494b4de8b7a47da8ee08f141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+--------------+\n",
      "|trip_id|route_id|tstamp             |destination   |\n",
      "+-------+--------+-------------------+--------------+\n",
      "|2000009|J       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000008|I       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000007|H       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000006|G       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000001|B       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000000|A       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000005|F       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000004|E       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000003|D       |2020-11-09 23:41:57|La Paz        |\n",
      "|2000002|C       |2020-11-09 23:41:57|La Paz        |\n",
      "|1999997|H       |2020-11-09 23:31:09|Rio de Janeiro|\n",
      "|1999998|I       |2020-11-09 23:31:09|Quito         |\n",
      "|1999999|J       |2020-11-09 23:31:09|Córdoba       |\n",
      "+-------+--------+-------------------+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Records.\n",
    "\n",
    "Apache Hudi supports implementing two types of deletes on data stored in Hudi datasets, by enabling the user to specify a different record payload implementation.\n",
    "\n",
    "* **Soft Deletes** : With soft deletes, user wants to retain the key but just null out the values for all other fields. This can be simply achieved by ensuring the appropriate fields are nullable in the dataset schema and simply upserting the dataset after setting these fields to null.\n",
    "    \n",
    "* **Hard Deletes** : A stronger form of delete is to physically remove any trace of the record from the dataset. \n",
    "\n",
    "Let's now execute some hard delete operations on our dataset which will remove the records from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's delete the 10 records with the \"Buenos Aires\" destination we added to the table. Note that the only change is the single line that set the hoodie.datasource.write.payload.class to org.apache.hudi.EmptyHoodieRecordPayload to delete the records.\n",
    "\n",
    "```\n",
    ".option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292a3ffa1d4f4fc6aa467d04690b22c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|     La Paz|       A|2000000|2020-11-09 23:41:57|\n",
      "|     La Paz|       B|2000001|2020-11-09 23:41:57|\n",
      "|     La Paz|       C|2000002|2020-11-09 23:41:57|\n",
      "|     La Paz|       D|2000003|2020-11-09 23:41:57|\n",
      "|     La Paz|       E|2000004|2020-11-09 23:41:57|\n",
      "|     La Paz|       F|2000005|2020-11-09 23:41:57|\n",
      "|     La Paz|       G|2000006|2020-11-09 23:41:57|\n",
      "|     La Paz|       H|2000007|2020-11-09 23:41:57|\n",
      "|     La Paz|       I|2000008|2020-11-09 23:41:57|\n",
      "|     La Paz|       J|2000009|2020-11-09 23:41:57|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab26fe187e04515822898518cbc2351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df5.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(PAYLOAD_CLASS_OPT_KEY, EMPTY_PAYLOAD_CLASS_OPT_VAL)\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88499ffb5c2845c6b3b88725050a76a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+--------------+\n",
      "|trip_id|route_id|tstamp             |destination   |\n",
      "+-------+--------+-------------------+--------------+\n",
      "|1999997|H       |2020-11-09 23:31:09|Rio de Janeiro|\n",
      "|1999998|I       |2020-11-09 23:31:09|Quito         |\n",
      "|1999999|J       |2020-11-09 23:31:09|Córdoba       |\n",
      "+-------+--------+-------------------+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the records > 2000000 no longer exist in our table.\n",
    "\n",
    "Let's observe the number of files in S3. Expected : 6 files (initial files (3) + one upsert + one insert + one delete = 6)\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/ --summarize --human-readable\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:11:39    0 Bytes .hoodie_$folder$\n",
    "2020-04-28 23:11:52   93 Bytes .hoodie_partition_metadata\n",
    "2020-04-28 23:15:43    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-51-609_20200428231528.parquet\n",
    "2020-04-28 23:11:59    4.8 MiB 0eeafaf2-0110-4056-b509-724b11a4c0b6-0_0-7-67_20200428231141.parquet\n",
    "2020-04-28 23:18:24    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-135-1376_20200428231814.parquet\n",
    "2020-04-28 23:17:17    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_0-93-1007_20200428231705.parquet\n",
    "2020-04-28 23:11:59    4.4 MiB 1ce9cbac-56e4-477e-a2bc-33fe62b3550f-0_1-7-68_20200428231141.parquet\n",
    "2020-04-28 23:11:59    4.6 MiB 579443dc-8f56-4990-bed6-a527f21e9682-0_2-7-69_20200428231141.parquet\n",
    "\n",
    "Total Objects: 8\n",
    "   Total Size: 27.5 MiB\n",
    "\n",
    "```\n",
    "\n",
    "In our example, we set number of commits to retain as 10. So, maximum only 10 new files can be created on top of our bulk insert files. i.e., 13 files in total. If we had set the commits_to_retain as 2, the number of files created will not increase beyond initial files(3) + commits_to_retain(2) = 5 files. This is because Hudi Cleaning Policy will delete older files when writing based on the commit retain policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "Let's say we want to roll back the last delete we made. \n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "```\n",
    "```\n",
    "connect --path s3://hudi-dms-repl-dmslabs3bucket-XXXXXX/workshop1/hudi_trips_table/\n",
    "```\n",
    "```\n",
    "commits show\n",
    "```\n",
    "\n",
    "hudi:hudi_trips_table->commits show\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231705.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231528.commit' for reading\n",
    "20/04/28 23:46:09 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/.hoodie/20200428231141.commit' for reading\n",
    "╔════════════════╤═════════════════════╤═══════════════════╤═════════════════════╤══════════════════════════╤═══════════════════════╤══════════════════════════════╤══════════════╗\n",
    "║ CommitTime     │ Total Bytes Written │ Total Files Added │ Total Files Updated │ Total Partitions Written │ Total Records Written │ Total Update Records Written │ Total Errors ║\n",
    "╠════════════════╪═════════════════════╪═══════════════════╪═════════════════════╪══════════════════════════╪═══════════════════════╪══════════════════════════════╪══════════════╣\n",
    "║ 20200428231814 │ 4.4 MB              │ 0                 │ 1                   │ 1                        │ 642132                │ 0                            │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "║ 20200428231528 │ 4.8 MB              │ 0                 │ 1                   │ 1                        │ 697329                │ 10                           │ 0            ║\n",
    "╟────────────────┼─────────────────────┼───────────────────┼─────────────────────┼──────────────────────────┼───────────────────────┼──────────────────────────────┼──────────────╢\n",
    "\n",
    "║ 20200428231141 │ 13.8 MB             │ 3                 │ 0                   │ 1                        │ 2000000               │ 0                            │ 0            ║\n",
    "╚════════════════╧═════════════════════╧═══════════════════╧═════════════════════╧══════════════════════════╧═══════════════════════╧══════════════════════════════╧══════════════╝\n",
    "\n",
    "```\n",
    "commit rollback --commit 20200428231814\n",
    "```\n",
    "    \n",
    "Now let us check what happened to the records we deleted earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf18e32315dd4b94b7bb13d737064cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------------------+--------------+\n",
      "|trip_id|route_id|tstamp             |destination   |\n",
      "+-------+--------+-------------------+--------------+\n",
      "|1999997|H       |2020-11-09 23:31:09|Rio de Janeiro|\n",
      "|1999998|I       |2020-11-09 23:31:09|Quito         |\n",
      "|1999999|J       |2020-11-09 23:31:09|Córdoba       |\n",
      "+-------+--------+-------------------+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, tstamp, destination from \"+config['table_name'] +\" where trip_id > 1999996\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel with Hudi \n",
    "\n",
    "Now, let us time travel with Hudi (query previous commits) with incremental and point-in-time queries\n",
    "\n",
    "First, lets check out incremental queries\n",
    "\n",
    "In EMR cluster, let us check Hudi CLI and check how commits look right now \n",
    "\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "connect --path s3://<Your S3 Bucket Here>/tmp/hudi/hudi_trips_table/\n",
    "\n",
    "commits show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbeb2a1d6b2244a48d44f8b8621cc174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|commitTime    |\n",
      "+--------------+\n",
      "|20201109233125|\n",
      "|20201109233933|\n",
      "+--------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_trips_table order by commitTime\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a971a74fea4faa83b384fbc96d7d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20201109233125')\n",
      "Row(commitTime='20201109233933')\n",
      "begin time: 20201109233125\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name| destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------------+--------+-------+-------------------+\n",
      "|     20201109233933|20201109233933_0_...|           1000000|                      |b4e750db-5360-447...|Buenos Aires|       A|1000000|2020-11-09 23:39:15|\n",
      "|     20201109233933|20201109233933_0_...|           1000001|                      |b4e750db-5360-447...|Buenos Aires|       B|1000001|2020-11-09 23:39:15|\n",
      "|     20201109233933|20201109233933_0_...|           1000002|                      |b4e750db-5360-447...|Buenos Aires|       C|1000002|2020-11-09 23:39:15|\n",
      "|     20201109233933|20201109233933_0_...|           1000003|                      |b4e750db-5360-447...|Buenos Aires|       D|1000003|2020-11-09 23:39:15|\n",
      "|     20201109233933|20201109233933_0_...|           1000004|                      |b4e750db-5360-447...|Buenos Aires|       E|1000004|2020-11-09 23:39:15|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+------------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "\n",
    "for elem in commits: \n",
    "    print(elem)\n",
    "\n",
    "beginTime = commits[-2][0] # commit time we are interested in\n",
    "print(\"begin time: \"+beginTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4fa38da1074fe283cd41e052410565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+------------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id| destination|             tstamp|\n",
      "+-------------------+-------+--------+------------+-------------------+\n",
      "|     20201109233933|1000000|       A|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000001|       B|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000002|       C|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000003|       D|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000004|       E|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000005|       F|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000006|       G|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000007|       H|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000008|       I|Buenos Aires|2020-11-09 23:39:15|\n",
      "|     20201109233933|1000009|       J|Buenos Aires|2020-11-09 23:39:15|\n",
      "+-------------------+-------+--------+------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets check out point-in-time queries. i.e., query from a specific commit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b723abd734154db18f22038deab4b129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commits: \n",
      "Row(commitTime='20201109233125')\n",
      "Row(commitTime='20201109233933')"
     ]
    }
   ],
   "source": [
    "commits = spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_trips_table order by commitTime\").collect()\n",
    "print(\"commits: \")\n",
    "for elem in commits: print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7b0a77e9b94088978d6a8b9cdf54e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end time: 20201109233125\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------------+--------+-------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|      destination|route_id|trip_id|             tstamp|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------------+--------+-------+-------------------+\n",
      "|     20201109233125|  20201109233125_2_2|            367003|                      |c0930ae0-9fca-4b8...|         Medellín|       D| 367003|2020-11-09 23:31:09|\n",
      "|     20201109233125|  20201109233125_2_4|            367004|                      |c0930ae0-9fca-4b8...|        Cartagena|       E| 367004|2020-11-09 23:31:09|\n",
      "|     20201109233125|  20201109233125_2_6|            367005|                      |c0930ae0-9fca-4b8...|Santiago de Chile|       F| 367005|2020-11-09 23:31:09|\n",
      "|     20201109233125|  20201109233125_2_8|            367006|                      |c0930ae0-9fca-4b8...|        Sao Paulo|       G| 367006|2020-11-09 23:31:09|\n",
      "|     20201109233125| 20201109233125_2_10|            367007|                      |c0930ae0-9fca-4b8...|   Rio de Janeiro|       H| 367007|2020-11-09 23:31:09|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-----------------+--------+-------+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#Lets check out the first state. i.e., before we updated the trip to Buenos Aires\n",
    "\n",
    "beginTime = \"000\" #Represents all commits > this time.\n",
    "endTime = commits[-2][0] # first commit\n",
    "print(\"end time: \"+endTime)\n",
    "\n",
    "# incrementally query data\n",
    "incViewDF = spark.read.format(\"org.apache.hudi\") \\\n",
    "                   .option(VIEW_TYPE_OPT_KEY, VIEW_TYPE_INCREMENTAL_OPT_VAL) \\\n",
    "                   .option(BEGIN_INSTANTTIME_OPT_KEY, beginTime) \\\n",
    "                   .option(END_INSTANTTIME_OPT_KEY, endTime) \\\n",
    "                   .load(config[\"target\"]) \n",
    "\n",
    "incViewDF.show(5)\n",
    "incViewDF.registerTempTable(\"hudi_incr_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02f076864bd429691f8290842ea4a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+-----------------+-------------------+\n",
      "|_hoodie_commit_time|trip_id|route_id|      destination|             tstamp|\n",
      "+-------------------+-------+--------+-----------------+-------------------+\n",
      "|     20201109233125| 999996|       G|        Sao Paulo|2020-11-09 23:31:09|\n",
      "|     20201109233125| 999997|       H|   Rio de Janeiro|2020-11-09 23:31:09|\n",
      "|     20201109233125| 999998|       I|            Quito|2020-11-09 23:31:09|\n",
      "|     20201109233125| 999999|       J|          Córdoba|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000000|       A|       Bogotá D.C|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000001|       B|             Lima|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000002|       C| Ciudad de México|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000003|       D|         Medellín|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000004|       E|        Cartagena|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000005|       F|Santiago de Chile|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000006|       G|        Sao Paulo|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000007|       H|   Rio de Janeiro|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000008|       I|            Quito|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000009|       J|          Córdoba|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000010|       A|       Bogotá D.C|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000011|       B|             Lima|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000012|       C| Ciudad de México|2020-11-09 23:31:09|\n",
      "|     20201109233125|1000013|       D|         Medellín|2020-11-09 23:31:09|\n",
      "+-------------------+-------+--------+-----------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select `_hoodie_commit_time`, trip_id, route_id, destination, tstamp from  hudi_incr_table where trip_id between 999996 and 1000013\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the records from 1000000 to 1000009 display first state of our table before we upserted the trip destination to La Paz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge On Read \n",
    "\n",
    "The default table type is Copy-On-Write which is best suited for read-heavy workloads with modest writes. Copy-On-Write creates commit files with original data + the new changes during writing itself. While this increases latency on writes, this set up makes it more manageable for faster read.\n",
    "\n",
    "For near real-time applications that mandate quick upserts, MERGE_ON_READ table type would be better suited. MOR table stores incoming upserts for each file group, onto a row based delta log (In Avro file format). This log is then merged with the existing Parquet file using a a compactor during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af7a5c93add483ab907a4f9071ac593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "\n",
    "S3_target_mor_prefix = \"workshop1/hudi_mor_trips_table\"\n",
    "\n",
    "config = {\n",
    "    \"table_name\": \"hudi_mor_trips_table\",\n",
    "    \"target\": (\"s3://{}/{}\".format(S3_bucket,S3_target_mor_prefix)),\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ac28a0a58148368a7c7a0a45b7b244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STORAGE_TYPE_OPT_KEY=\"hoodie.datasource.write.storage.type\"\n",
    "COMPACTION_INLINE_OPT_KEY=\"hoodie.compact.inline\"\n",
    "COMPACTION_MAX_DELTA_COMMITS_OPT_KEY=\"hoodie.compact.inline.max.delta.commits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d95852c191d49178bfc7afb6c484358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n",
      "+-----------------+--------+-------+-------------------+\n",
      "|      destination|route_id|trip_id|             tstamp|\n",
      "+-----------------+--------+-------+-------------------+\n",
      "|       Bogotá D.C|       A|      0|2020-11-10 00:05:06|\n",
      "|             Lima|       B|      1|2020-11-10 00:05:06|\n",
      "| Ciudad de México|       C|      2|2020-11-10 00:05:06|\n",
      "|         Medellín|       D|      3|2020-11-10 00:05:06|\n",
      "|        Cartagena|       E|      4|2020-11-10 00:05:06|\n",
      "|Santiago de Chile|       F|      5|2020-11-10 00:05:06|\n",
      "|        Sao Paulo|       G|      6|2020-11-10 00:05:06|\n",
      "|   Rio de Janeiro|       H|      7|2020-11-10 00:05:06|\n",
      "|            Quito|       I|      8|2020-11-10 00:05:06|\n",
      "|          Córdoba|       J|      9|2020-11-10 00:05:06|\n",
      "|       Bogotá D.C|       A|     10|2020-11-10 00:05:06|\n",
      "|             Lima|       B|     11|2020-11-10 00:05:06|\n",
      "| Ciudad de México|       C|     12|2020-11-10 00:05:06|\n",
      "|         Medellín|       D|     13|2020-11-10 00:05:06|\n",
      "|        Cartagena|       E|     14|2020-11-10 00:05:06|\n",
      "|Santiago de Chile|       F|     15|2020-11-10 00:05:06|\n",
      "|        Sao Paulo|       G|     16|2020-11-10 00:05:06|\n",
      "|   Rio de Janeiro|       H|     17|2020-11-10 00:05:06|\n",
      "|            Quito|       I|     18|2020-11-10 00:05:06|\n",
      "|          Córdoba|       J|     19|2020-11-10 00:05:06|\n",
      "+-----------------+--------+-------+-------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "mor_dest = [\"Bogotá D.C\", \"Lima\", \"Ciudad de México\", \"Medellín\", \"Cartagena\", \"Santiago de Chile\",\"Sao Paulo\",\"Rio de Janeiro\",\"Quito\",\"Córdoba\"]\n",
    "df2 = create_json_df(spark, get_json_data(0, 2000000, dest))\n",
    "print(df1.count())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk insert will take the same time as COW as this is the first write "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47e0866018943ea92e758c74f5a1e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df2.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 3)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)\n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the number of files \n",
    "\n",
    "Let us check the contents of S3 path. Bulk insert operation on Copy-On-Write and Merge-On-Read tables is identical in terms of performance. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "```\n",
    "\n",
    "Notice the delta commits \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/.hoodie/\n",
    "2020-04-28 23:30:21          0 .aux_$folder$\n",
    "2020-04-28 23:30:21          0 .temp_$folder$\n",
    "2020-04-28 23:30:37       1077 20200428233020.clean\n",
    "2020-04-28 23:30:36       4929 20200428233020.deltacommit\n",
    "2020-04-28 23:30:21          0 archived_$folder$\n",
    "2020-04-28 23:30:21        264 hoodie.properties\n",
    "```\n",
    "\n",
    "This is the first commit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try to upsert some records into MOR table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c21459186f4d57b090a458f68582d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------------------+\n",
      "|destination|route_id|trip_id|             tstamp|\n",
      "+-----------+--------+-------+-------------------+\n",
      "|  San Diego|       A|1000000|2020-11-10 00:12:50|\n",
      "|  San Diego|       B|1000001|2020-11-10 00:12:50|\n",
      "|  San Diego|       C|1000002|2020-11-10 00:12:50|\n",
      "|  San Diego|       D|1000003|2020-11-10 00:12:50|\n",
      "|  San Diego|       E|1000004|2020-11-10 00:12:50|\n",
      "|  San Diego|       F|1000005|2020-11-10 00:12:50|\n",
      "|  San Diego|       G|1000006|2020-11-10 00:12:50|\n",
      "|  San Diego|       H|1000007|2020-11-10 00:12:50|\n",
      "|  San Diego|       I|1000008|2020-11-10 00:12:50|\n",
      "|  San Diego|       J|1000009|2020-11-10 00:12:50|\n",
      "+-----------+--------+-------+-------------------+"
     ]
    }
   ],
   "source": [
    "upsert_dest = [\"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\", \"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\",\"San Diego\"]\n",
    "df3 = create_json_df(spark, get_json_data(1000000, 10, upsert_dest))\n",
    "df3.count()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eefe2e575bb4bc59ca261cae9cfe533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df3.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, UPSERT_OPERATION_OPT_VAL)\n",
    "      .option(UPSERT_PARALLELISM, 20)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HUDI_CLEANER_POLICY, KEEP_LATEST_COMMITS)\n",
    "      .option(HUDI_COMMITS_RETAINED,config[\"commits_to_retain\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,NONPARTITION_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(KEYGENERATOR_CLASS_OPT_KEY,NONPARTITIONED_KEYGENERATOR_CLASS_OPT_VAL)  \n",
    "      .option(STORAGE_TYPE_OPT_KEY, \"MERGE_ON_READ\")\n",
    "      .option(COMPACTION_INLINE_OPT_KEY, \"false\")\n",
    "      .option(COMPACTION_MAX_DELTA_COMMITS_OPT_KEY, \"20\")\n",
    "      .mode(\"Append\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea4acc1bd314aef9bc6c02332cf3140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------------+-------------------+\n",
      "|trip_id|route_id|destination      |tstamp             |\n",
      "+-------+--------+-----------------+-------------------+\n",
      "|999996 |G       |Sao Paulo        |2020-11-10 00:05:06|\n",
      "|999997 |H       |Rio de Janeiro   |2020-11-10 00:05:06|\n",
      "|999998 |I       |Quito            |2020-11-10 00:05:06|\n",
      "|999999 |J       |Córdoba          |2020-11-10 00:05:06|\n",
      "|1000000|A       |Bogotá D.C       |2020-11-10 00:05:06|\n",
      "|1000001|B       |Lima             |2020-11-10 00:05:06|\n",
      "|1000002|C       |Ciudad de México |2020-11-10 00:05:06|\n",
      "|1000003|D       |Medellín         |2020-11-10 00:05:06|\n",
      "|1000004|E       |Cartagena        |2020-11-10 00:05:06|\n",
      "|1000005|F       |Santiago de Chile|2020-11-10 00:05:06|\n",
      "|1000006|G       |Sao Paulo        |2020-11-10 00:05:06|\n",
      "|1000007|H       |Rio de Janeiro   |2020-11-10 00:05:06|\n",
      "|1000008|I       |Quito            |2020-11-10 00:05:06|\n",
      "|1000009|J       |Córdoba          |2020-11-10 00:05:06|\n",
      "|1000010|A       |Bogotá D.C       |2020-11-10 00:05:06|\n",
      "+-------+--------+-----------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_ro where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets query the real-time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04d697a81974b56b68e7898a4626e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------+-------------------+\n",
      "|trip_id|route_id|destination   |tstamp             |\n",
      "+-------+--------+--------------+-------------------+\n",
      "|999996 |G       |Sao Paulo     |2020-11-10 00:05:06|\n",
      "|999997 |H       |Rio de Janeiro|2020-11-10 00:05:06|\n",
      "|999998 |I       |Quito         |2020-11-10 00:05:06|\n",
      "|999999 |J       |Córdoba       |2020-11-10 00:05:06|\n",
      "|1000000|A       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000001|B       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000002|C       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000003|D       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000004|E       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000005|F       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000006|G       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000007|H       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000008|I       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000009|J       |San Diego     |2020-11-10 00:12:50|\n",
      "|1000010|A       |Bogotá D.C    |2020-11-10 00:05:06|\n",
      "+-------+--------+--------------+-------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select trip_id, route_id, destination, tstamp from \"+config['table_name']+\"_rt where trip_id between 999996 and 1000010\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the S3 path again. There is no change in number of Parquet files after upsert operation unlike Copy-On-Write tables. \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n",
    "\n",
    "Now, let us compact using Hudi CLI \n",
    "\n",
    "On EMR:\n",
    "\n",
    "```\n",
    "/usr/lib/hudi/cli/bin/hudi-cli.sh\n",
    "\n",
    "hudi->connect --path s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "╔═════════════════════════╤═══════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╧═══════╧═══════════════════════════════╣\n",
    "║ (empty)                                                         ║\n",
    "╚═════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "```\n",
    "\n",
    "Notice there are no compactions for our mor table. Let us manually schedule and run a compaction. \n",
    "\n",
    "```\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction schedule\n",
    "Compaction successfully completed for 20200428233601\n",
    "\n",
    "\n",
    "hudi:hudi_mor_trips_table->connect --path s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "\n",
    "hudi:hudi_mor_trips_table->compactions show all\n",
    "20/04/28 23:39:04 INFO timeline.HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@240d2f9d\n",
    "20/04/28 23:39:04 INFO s3n.S3NativeFileSystem: Opening 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/.hoodie/.aux/20200428233601.compaction.requested' for reading\n",
    "╔═════════════════════════╤═══════════╤═══════════════════════════════╗\n",
    "║ Compaction Instant Time │ State     │ Total FileIds to be Compacted ║\n",
    "╠═════════════════════════╪═══════════╪═══════════════════════════════╣\n",
    "║ 20200428233601          │ REQUESTED │ 1                             ║\n",
    "╚═════════════════════════╧═══════════╧═══════════════════════════════╝\n",
    "\n",
    "hudi:hudi_mor_trips_table->compaction run --parallelism 10 --sparkMemory 4G --retry 2 --schemaFilePath s3://<Your S3 Bucket Here>/schema.avsc --compactionInstant 20200428233601\n",
    "\n",
    "Compaction successfully completed for 20200428233601\n",
    "\n",
    "```\n",
    "\n",
    "You can also schedule a compaction using \"compaction schedule\" option. \n",
    "\n",
    "Now, if we check the S3 path, we will see the delta commit has been merged into a new file \n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_mor_trips_table/\n",
    "                           PRE .hoodie/\n",
    "2020-04-28 23:33:22       2071 .ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_20200428233020.log.1_0-227-3837\n",
    "2020-04-28 23:30:20          0 .hoodie_$folder$\n",
    "2020-04-28 23:30:26         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:30:33    4378000 45b1ce07-f9ac-496d-8b03-20af011a0c44-0_1-194-3566_20200428233020.parquet\n",
    "2020-04-28 23:30:34    5048941 932d5e97-c5f0-4c91-a7f6-f65d487a5e2b-0_2-194-3567_20200428233020.parquet\n",
    "2020-04-28 23:41:48    5065205 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-0-0_20200428233601.parquet\n",
    "2020-04-28 23:30:34    5065824 ea6e8bfa-e70c-4f7e-90ec-37d018fb0acf-0_0-194-3565_20200428233020.parquet\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Tables\n",
    "\n",
    "Let's do the same thing with Partitioned Tables. For the sake of this demo, we will be making route_id as partition field. You can also have a nested partition structure like yyyy/mm/dd which is more common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a53de7e51f4631ab1fdfb845700ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## CHANGE ME ##\n",
    "config = {\n",
    "    \"table_name\": \"hudi_partitioned_trips_table\",\n",
    "    \"target\": \"s3://<Your S3 Bucket Here>/tmp/hudi/hudi_partitioned_trips_table\",\n",
    "    \"primary_key\": \"trip_id\",\n",
    "    \"sort_key\": \"tstamp\",\n",
    "    \"commits_to_retain\": \"2\",\n",
    "    \"partition_keys\" : \"route_id\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0df9b335b334675bc997cc4771d67af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part_dest = [\"Seattle\", \"New York\", \"New Jersey\", \"Los Angeles\", \"Las Vegas\", \"Tucson\",\"Washington DC\",\"Philadelphia\",\"Miami\",\"San Francisco\"]\n",
    "df1 = create_json_df(spark, get_json_data(0, 2000000, part_dest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the partitionKey column to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19201c0eee384eacba25074d8f7dd2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  route_id|\n",
      "+----------+\n",
      "|route_id=A|\n",
      "|route_id=B|\n",
      "|route_id=C|\n",
      "|route_id=D|\n",
      "|route_id=E|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "hudiTablePartitionKey=\"route_id\"\n",
    "df1 = df1.withColumn(hudiTablePartitionKey,concat(lit(\"route_id=\"),col(\"route_id\")))\n",
    "df1.select(hudiTablePartitionKey).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now write out the data to S3. Notice that the Hive Partition Extractor class has changed in the statement below:\n",
    "\n",
    "```\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105b6994cf8d42fd9bb8354be765b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(df1.write.format(HUDI_FORMAT)\n",
    "      .option(PRECOMBINE_FIELD_OPT_KEY, config[\"sort_key\"])\n",
    "      .option(RECORDKEY_FIELD_OPT_KEY, config[\"primary_key\"])\n",
    "      .option(TABLE_NAME, config['table_name'])\n",
    "      .option(OPERATION_OPT_KEY, BULK_INSERT_OPERATION_OPT_VAL)\n",
    "      .option(BULK_INSERT_PARALLELISM, 6)\n",
    "      .option(S3_CONSISTENCY_CHECK, \"true\")\n",
    "      .option(HIVE_PARTITION_FIELDS_OPT_KEY, config[\"partition_keys\"])\n",
    "      .option(HIVE_TABLE_OPT_KEY,config['table_name'])\n",
    "      .option(HIVE_SYNC_ENABLED_OPT_KEY,\"true\")\n",
    "      .option(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY,MULTIPART_KEYS_EXTRACTOR_CLASS_OPT_VAL)\n",
    "      .option(PARTITIONPATH_FIELD_OPT_KEY,\"route_id\")\n",
    "      .mode(\"Overwrite\")\n",
    "      .save(config['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd43a5b1e422416b88bb4f9763f4b307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|createtab_stmt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|CREATE EXTERNAL TABLE `hudi_partitioned_trips_table`(`_hoodie_commit_time` STRING, `_hoodie_commit_seqno` STRING, `_hoodie_record_key` STRING, `_hoodie_partition_path` STRING, `_hoodie_file_name` STRING, `destination` STRING, `trip_id` BIGINT, `tstamp` STRING)\n",
      "PARTITIONED BY (`route_id` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\n",
      "LOCATION 's3://<Your S3 Bucket Here>/tmp/hudi/hudi_partitioned_trips_table'\n",
      "TBLPROPERTIES (\n",
      "  'last_commit_time_sync' = '20200428234249',\n",
      "  'transient_lastDdlTime' = '1588117387'\n",
      ")\n",
      "|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show create table \"+config['table_name']).show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the partitions fields are present in our Hive table. \n",
    "\n",
    "```\n",
    "PARTITIONED BY (`route_id` STRING)\n",
    "```\n",
    "\n",
    "Let's now query the data and group by the the partition columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e21fcd5a8f74018b6bdb25c13be20bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|route_id|num_trips|\n",
      "+--------+---------+\n",
      "|A       |200000   |\n",
      "|B       |200000   |\n",
      "|C       |200000   |\n",
      "|D       |200000   |\n",
      "|E       |200000   |\n",
      "|F       |200000   |\n",
      "|G       |200000   |\n",
      "|H       |200000   |\n",
      "|I       |200000   |\n",
      "|J       |200000   |\n",
      "+--------+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select route_id, count(*) as num_trips from \"+config['table_name']+\" group by route_id order by route_id\").show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the S3 path\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_partitioned_trips_table/\n",
    "                           PRE .hoodie/\n",
    "                           PRE route_id=A/\n",
    "                           PRE route_id=B/\n",
    "                           PRE route_id=C/\n",
    "                           PRE route_id=D/\n",
    "                           PRE route_id=E/\n",
    "                           PRE route_id=F/\n",
    "                           PRE route_id=G/\n",
    "                           PRE route_id=H/\n",
    "                           PRE route_id=I/\n",
    "                           PRE route_id=J/\n",
    "2020-04-28 23:42:50          0 .hoodie_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=A_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=B_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=C_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=D_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=E_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=F_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=G_$folder$\n",
    "2020-04-28 23:42:57          0 route_id=H_$folder$\n",
    "2020-04-28 23:42:55          0 route_id=I_$folder$\n",
    "2020-04-28 23:42:58          0 route_id=J_$folder$\n",
    "\n",
    "```\n",
    "\n",
    "Under each partition, there will be partition metadata\n",
    "\n",
    "```\n",
    "\n",
    "$ aws s3 ls s3://<Your S3 Bucket Here>/tmp/hudi/hudi_partitioned_trips_table/route_id=A/\n",
    "2020-04-28 23:42:56         93 .hoodie_partition_metadata\n",
    "2020-04-28 23:42:59    1723564 447b86e6-500c-463a-bdac-74abb867efad-0_0-256-4156_20200428234249.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other operations Upsert etc. behave the same way on Partitioned tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
